{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "8tQJd2YSCfWR"
   },
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "D7tqLMoKF6uq"
   },
   "source": [
    "Deep Learning\n",
    "=============\n",
    "\n",
    "Assignment 6\n",
    "------------\n",
    "\n",
    "After training a skip-gram model in `5_word2vec.ipynb`, the goal of this notebook is to train a LSTM character model over [Text8](http://mattmahoney.net/dc/textdata) data.\n",
    "\n",
    "This assignment deals with recurrent neural network and word2vec model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "cellView": "both",
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "collapsed": true,
    "id": "MvEblsgEXxrd"
   },
   "outputs": [],
   "source": [
    "# These are all the modules we'll be using later. Make sure you can import them\n",
    "# before proceeding further.\n",
    "from __future__ import print_function\n",
    "import os\n",
    "import numpy as np\n",
    "import random\n",
    "import string\n",
    "import tensorflow as tf\n",
    "import zipfile\n",
    "from six.moves import range\n",
    "from six.moves.urllib.request import urlretrieve"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "cellView": "both",
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     },
     "output_extras": [
      {
       "item_id": 1
      }
     ]
    },
    "colab_type": "code",
    "collapsed": false,
    "executionInfo": {
     "elapsed": 5993,
     "status": "ok",
     "timestamp": 1445965582896,
     "user": {
      "color": "#1FA15D",
      "displayName": "Vincent Vanhoucke",
      "isAnonymous": false,
      "isMe": true,
      "permissionId": "05076109866853157986",
      "photoUrl": "//lh6.googleusercontent.com/-cCJa7dTDcgQ/AAAAAAAAAAI/AAAAAAAACgw/r2EZ_8oYer4/s50-c-k-no/photo.jpg",
      "sessionId": "6f6f07b359200c46",
      "userId": "102167687554210253930"
     },
     "user_tz": 420
    },
    "id": "RJ-o3UBUFtCw",
    "outputId": "d530534e-0791-4a94-ca6d-1c8f1b908a9e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found and verified text8.zip\n"
     ]
    }
   ],
   "source": [
    "url = 'http://mattmahoney.net/dc/'\n",
    "\n",
    "def maybe_download(filename, expected_bytes):\n",
    "  \"\"\"Download a file if not present, and make sure it's the right size.\"\"\"\n",
    "  if not os.path.exists(filename):\n",
    "    filename, _ = urlretrieve(url + filename, filename)\n",
    "  statinfo = os.stat(filename)\n",
    "  if statinfo.st_size == expected_bytes:\n",
    "    print('Found and verified %s' % filename)\n",
    "  else:\n",
    "    print(statinfo.st_size)\n",
    "    raise Exception(\n",
    "      'Failed to verify ' + filename + '. Can you get to it with a browser?')\n",
    "  return filename\n",
    "\n",
    "filename = maybe_download('text8.zip', 31344016)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "cellView": "both",
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     },
     "output_extras": [
      {
       "item_id": 1
      }
     ]
    },
    "colab_type": "code",
    "collapsed": false,
    "executionInfo": {
     "elapsed": 5982,
     "status": "ok",
     "timestamp": 1445965582916,
     "user": {
      "color": "#1FA15D",
      "displayName": "Vincent Vanhoucke",
      "isAnonymous": false,
      "isMe": true,
      "permissionId": "05076109866853157986",
      "photoUrl": "//lh6.googleusercontent.com/-cCJa7dTDcgQ/AAAAAAAAAAI/AAAAAAAACgw/r2EZ_8oYer4/s50-c-k-no/photo.jpg",
      "sessionId": "6f6f07b359200c46",
      "userId": "102167687554210253930"
     },
     "user_tz": 420
    },
    "id": "Mvf09fjugFU_",
    "outputId": "8f75db58-3862-404b-a0c3-799380597390"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data size 100000000\n"
     ]
    }
   ],
   "source": [
    "def read_data(filename):\n",
    "  f = zipfile.ZipFile(filename)\n",
    "  for name in f.namelist():\n",
    "    return tf.compat.as_str(f.read(name))\n",
    "  f.close()\n",
    "  \n",
    "text = read_data(filename)\n",
    "print('Data size %d' % len(text))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ga2CYACE-ghb"
   },
   "source": [
    "Create a small validation set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "cellView": "both",
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     },
     "output_extras": [
      {
       "item_id": 1
      }
     ]
    },
    "colab_type": "code",
    "collapsed": false,
    "executionInfo": {
     "elapsed": 6184,
     "status": "ok",
     "timestamp": 1445965583138,
     "user": {
      "color": "#1FA15D",
      "displayName": "Vincent Vanhoucke",
      "isAnonymous": false,
      "isMe": true,
      "permissionId": "05076109866853157986",
      "photoUrl": "//lh6.googleusercontent.com/-cCJa7dTDcgQ/AAAAAAAAAAI/AAAAAAAACgw/r2EZ_8oYer4/s50-c-k-no/photo.jpg",
      "sessionId": "6f6f07b359200c46",
      "userId": "102167687554210253930"
     },
     "user_tz": 420
    },
    "id": "w-oBpfFG-j43",
    "outputId": "bdb96002-d021-4379-f6de-a977924f0d02"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "99999000 ons anarchists advocate social relations based upon voluntary as\n",
      "1000  anarchism originated as a term of abuse first used against earl\n"
     ]
    }
   ],
   "source": [
    "valid_size = 1000\n",
    "valid_text = text[:valid_size]\n",
    "train_text = text[valid_size:]\n",
    "train_size = len(train_text)\n",
    "print(train_size, train_text[:64])\n",
    "print(valid_size, valid_text[:64])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'abcdefghijklmnopqrstuvwxyz'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "string.ascii_lowercase"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Zdw6i4F8glpp"
   },
   "source": [
    "Utility functions to map characters to vocabulary IDs and back."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "cellView": "both",
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     },
     "output_extras": [
      {
       "item_id": 1
      }
     ]
    },
    "colab_type": "code",
    "collapsed": false,
    "executionInfo": {
     "elapsed": 6276,
     "status": "ok",
     "timestamp": 1445965583249,
     "user": {
      "color": "#1FA15D",
      "displayName": "Vincent Vanhoucke",
      "isAnonymous": false,
      "isMe": true,
      "permissionId": "05076109866853157986",
      "photoUrl": "//lh6.googleusercontent.com/-cCJa7dTDcgQ/AAAAAAAAAAI/AAAAAAAACgw/r2EZ_8oYer4/s50-c-k-no/photo.jpg",
      "sessionId": "6f6f07b359200c46",
      "userId": "102167687554210253930"
     },
     "user_tz": 420
    },
    "id": "gAL1EECXeZsD",
    "outputId": "88fc9032-feb9-45ff-a9a0-a26759cc1f2e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unexpected character: ï\n",
      "1 26 0 0\n",
      "a z  \n"
     ]
    }
   ],
   "source": [
    "vocabulary_size = len(string.ascii_lowercase) + 1 # [a-z] + ' '\n",
    "first_letter = ord(string.ascii_lowercase[0])\n",
    "\n",
    "def char2id(char):\n",
    "  if char in string.ascii_lowercase:\n",
    "    return ord(char) - first_letter + 1\n",
    "  elif char == ' ':\n",
    "    return 0\n",
    "  else:\n",
    "    print('Unexpected character: %s' % char)\n",
    "    return 0\n",
    "  \n",
    "def id2char(dictid):\n",
    "  if dictid > 0:\n",
    "    return chr(dictid + first_letter - 1)\n",
    "  else:\n",
    "    return ' '\n",
    "\n",
    "print(char2id('a'), char2id('z'), char2id(' '), char2id('ï'))\n",
    "print(id2char(1), id2char(26), id2char(0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "lFwoyygOmWsL"
   },
   "source": [
    "Function to generate a training batch for the LSTM model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "cellView": "both",
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     },
     "output_extras": [
      {
       "item_id": 1
      }
     ]
    },
    "colab_type": "code",
    "collapsed": false,
    "executionInfo": {
     "elapsed": 6473,
     "status": "ok",
     "timestamp": 1445965583467,
     "user": {
      "color": "#1FA15D",
      "displayName": "Vincent Vanhoucke",
      "isAnonymous": false,
      "isMe": true,
      "permissionId": "05076109866853157986",
      "photoUrl": "//lh6.googleusercontent.com/-cCJa7dTDcgQ/AAAAAAAAAAI/AAAAAAAACgw/r2EZ_8oYer4/s50-c-k-no/photo.jpg",
      "sessionId": "6f6f07b359200c46",
      "userId": "102167687554210253930"
     },
     "user_tz": 420
    },
    "id": "d9wMtjy5hCj9",
    "outputId": "3dd79c80-454a-4be0-8b71-4a4a357b3367"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['ons anarchi', 'when milita', 'lleria arch', ' abbeys and', 'married urr', 'hel and ric', 'y and litur', 'ay opened f', 'tion from t', 'migration t', 'new york ot', 'he boeing s', 'e listed wi', 'eber has pr', 'o be made t', 'yer who rec', 'ore signifi', 'a fierce cr', ' two six ei', 'aristotle s', 'ity can be ', ' and intrac', 'tion of the', 'dy to pass ', 'f certain d', 'at it will ', 'e convince ', 'ent told hi', 'ampaign and', 'rver side s', 'ious texts ', 'o capitaliz', 'a duplicate', 'gh ann es d', 'ine january', 'ross zero t', 'cal theorie', 'ast instanc', ' dimensiona', 'most holy m', 't s support', 'u is still ', 'e oscillati', 'o eight sub', 'of italy la', 's the tower', 'klahoma pre', 'erprise lin', 'ws becomes ', 'et in a naz', 'the fabian ', 'etchy to re', ' sharman ne', 'ised empero', 'ting in pol', 'd neo latin', 'th risky ri', 'encyclopedi', 'fense the a', 'duating fro', 'treet grid ', 'ations more', 'appeal of d', 'si have mad']\n",
      "['ists advoca', 'ary governm', 'hes nationa', 'd monasteri', 'raca prince', 'chard baer ', 'rgical lang', 'for passeng', 'the nationa', 'took place ', 'ther well k', 'seven six s', 'ith a gloss', 'robably bee', 'to recogniz', 'ceived the ', 'icant than ', 'ritic of th', 'ight in sig', 's uncaused ', ' lost as in', 'cellular ic', 'e size of t', ' him a stic', 'drugs confu', ' take to co', ' the priest', 'im to name ', 'd barred at', 'standard fo', ' such as es', 'ze on the g', 'e of the or', 'd hiver one', 'y eight mar', 'the lead ch', 'es classica', 'ce the non ', 'al analysis', 'mormons bel', 't or at lea', ' disagreed ', 'ing system ', 'btypes base', 'anguages th', 'r commissio', 'ess one nin', 'nux suse li', ' the first ', 'zi concentr', ' society ne', 'elatively s', 'etworks sha', 'or hirohito', 'litical ini', 'n most of t', 'iskerdoo ri', 'ic overview', 'air compone', 'om acnm acc', ' centerline', 'e than any ', 'devotional ', 'de such dev']\n",
      "[' a']\n",
      "['an']\n"
     ]
    }
   ],
   "source": [
    "batch_size=64\n",
    "num_unrollings=10\n",
    "\n",
    "class BatchGenerator(object):\n",
    "  def __init__(self, text, batch_size, num_unrollings):\n",
    "    self._text = text\n",
    "    self._text_size = len(text)\n",
    "    self._batch_size = batch_size\n",
    "    self._num_unrollings = num_unrollings\n",
    "    segment = self._text_size // batch_size\n",
    "    self._cursor = [ offset * segment for offset in range(batch_size)]\n",
    "    self._last_batch = self._next_batch()\n",
    "  \n",
    "  def _next_batch(self):\n",
    "    \"\"\"Generate a single batch from the current cursor position in the data.\"\"\"\n",
    "    batch = np.zeros(shape=(self._batch_size, vocabulary_size), dtype=np.float)\n",
    "    for b in range(self._batch_size):\n",
    "      batch[b, char2id(self._text[self._cursor[b]])] = 1.0\n",
    "      self._cursor[b] = (self._cursor[b] + 1) % self._text_size\n",
    "    return batch\n",
    "  \n",
    "  def next(self):\n",
    "    \"\"\"Generate the next array of batches from the data. The array consists of\n",
    "    the last batch of the previous array, followed by num_unrollings new ones.\n",
    "    \"\"\"\n",
    "    batches = [self._last_batch]\n",
    "    for step in range(self._num_unrollings):\n",
    "      batches.append(self._next_batch())\n",
    "    self._last_batch = batches[-1]\n",
    "    return batches\n",
    "\n",
    "def characters(probabilities):\n",
    "  \"\"\"Turn a 1-hot encoding or a probability distribution over the possible\n",
    "  characters back into its (most likely) character representation.\"\"\"\n",
    "  return [id2char(c) for c in np.argmax(probabilities, 1)]\n",
    "\n",
    "def batches2string(batches):\n",
    "  \"\"\"Convert a sequence of batches back into their (most likely) string\n",
    "  representation.\"\"\"\n",
    "  s = [''] * batches[0].shape[0]\n",
    "  for b in batches:\n",
    "    s = [''.join(x) for x in zip(s, characters(b))]\n",
    "  return s\n",
    "\n",
    "train_batches = BatchGenerator(train_text, batch_size, num_unrollings)\n",
    "valid_batches = BatchGenerator(valid_text, 1, 1)\n",
    "\n",
    "print(batches2string(train_batches.next()))\n",
    "print(batches2string(train_batches.next()))\n",
    "print(batches2string(valid_batches.next()))\n",
    "print(batches2string(valid_batches.next()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "cellView": "both",
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "collapsed": true,
    "id": "KyVd8FxT5QBc"
   },
   "outputs": [],
   "source": [
    "def logprob(predictions, labels):\n",
    "  \"\"\"Log-probability of the true labels in a predicted batch.\"\"\"\n",
    "  predictions[predictions < 1e-10] = 1e-10\n",
    "  return np.sum(np.multiply(labels, -np.log(predictions))) / labels.shape[0]\n",
    "\n",
    "def sample_distribution(distribution):\n",
    "  \"\"\"Sample one element from a distribution assumed to be an array of normalized\n",
    "  probabilities.\n",
    "  \"\"\"\n",
    "  r = random.uniform(0, 1)\n",
    "  s = 0\n",
    "  for i in range(len(distribution)):\n",
    "    s += distribution[i]\n",
    "    if s >= r:\n",
    "      return i\n",
    "  return len(distribution) - 1\n",
    "\n",
    "def sample(prediction):\n",
    "  \"\"\"Turn a (column) prediction into 1-hot encoded samples.\"\"\"\n",
    "  p = np.zeros(shape=[1, vocabulary_size], dtype=np.float)\n",
    "  p[0, sample_distribution(prediction[0])] = 1.0\n",
    "  return p\n",
    "\n",
    "def random_distribution():\n",
    "  \"\"\"Generate a random column of probabilities.\"\"\"\n",
    "  b = np.random.uniform(0.0, 1.0, size=[1, vocabulary_size])\n",
    "  return b/np.sum(b, 1)[:,None]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "K8f67YXaDr4C"
   },
   "source": [
    "Simple LSTM Model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "cellView": "both",
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "collapsed": false,
    "id": "Q5rxZK6RDuGe"
   },
   "outputs": [],
   "source": [
    "num_nodes = 64\n",
    "\n",
    "graph = tf.Graph()\n",
    "with graph.as_default():\n",
    "  \n",
    "  # Parameters:\n",
    "  # Input gate: input, previous output, and bias.\n",
    "  ix = tf.Variable(tf.truncated_normal([vocabulary_size, num_nodes], -0.1, 0.1))\n",
    "  im = tf.Variable(tf.truncated_normal([num_nodes, num_nodes], -0.1, 0.1))\n",
    "  ib = tf.Variable(tf.zeros([1, num_nodes]))\n",
    "  # Forget gate: input, previous output, and bias.\n",
    "  fx = tf.Variable(tf.truncated_normal([vocabulary_size, num_nodes], -0.1, 0.1))\n",
    "  fm = tf.Variable(tf.truncated_normal([num_nodes, num_nodes], -0.1, 0.1))\n",
    "  fb = tf.Variable(tf.zeros([1, num_nodes]))\n",
    "  # Memory cell: input, state and bias.                             \n",
    "  cx = tf.Variable(tf.truncated_normal([vocabulary_size, num_nodes], -0.1, 0.1))\n",
    "  cm = tf.Variable(tf.truncated_normal([num_nodes, num_nodes], -0.1, 0.1))\n",
    "  cb = tf.Variable(tf.zeros([1, num_nodes]))\n",
    "  # Output gate: input, previous output, and bias.\n",
    "  ox = tf.Variable(tf.truncated_normal([vocabulary_size, num_nodes], -0.1, 0.1))\n",
    "  om = tf.Variable(tf.truncated_normal([num_nodes, num_nodes], -0.1, 0.1))\n",
    "  ob = tf.Variable(tf.zeros([1, num_nodes]))\n",
    "  # Variables saving state across unrollings.\n",
    "  saved_output = tf.Variable(tf.zeros([batch_size, num_nodes]), trainable=False)\n",
    "  saved_state = tf.Variable(tf.zeros([batch_size, num_nodes]), trainable=False)\n",
    "  # Classifier weights and biases.\n",
    "  w = tf.Variable(tf.truncated_normal([num_nodes, vocabulary_size], -0.1, 0.1))\n",
    "  b = tf.Variable(tf.zeros([vocabulary_size]))\n",
    "  \n",
    "  # Definition of the cell computation.\n",
    "  def lstm_cell(i, o, state):\n",
    "    \"\"\"Create a LSTM cell. See e.g.: http://arxiv.org/pdf/1402.1128v1.pdf\n",
    "    Note that in this formulation, we omit the various connections between the\n",
    "    previous state and the gates.\"\"\"\n",
    "    input_gate = tf.sigmoid(tf.matmul(i, ix) + tf.matmul(o, im) + ib)\n",
    "    forget_gate = tf.sigmoid(tf.matmul(i, fx) + tf.matmul(o, fm) + fb)\n",
    "    update = tf.matmul(i, cx) + tf.matmul(o, cm) + cb\n",
    "    state = forget_gate * state + input_gate * tf.tanh(update)\n",
    "    output_gate = tf.sigmoid(tf.matmul(i, ox) + tf.matmul(o, om) + ob)\n",
    "    return output_gate * tf.tanh(state), state\n",
    "\n",
    "  # Input data.\n",
    "  train_data = list()\n",
    "  for _ in range(num_unrollings + 1):\n",
    "    train_data.append(\n",
    "      tf.placeholder(tf.float32, shape=[batch_size,vocabulary_size]))\n",
    "  train_inputs = train_data[:num_unrollings]\n",
    "  train_labels = train_data[1:]  # labels are inputs shifted by one time step.\n",
    "\n",
    "  # Unrolled LSTM loop.\n",
    "  outputs = list()\n",
    "  output = saved_output\n",
    "  state = saved_state\n",
    "  for i in train_inputs:\n",
    "    output, state = lstm_cell(i, output, state)\n",
    "    outputs.append(output)\n",
    "\n",
    "  # State saving across unrollings.\n",
    "  with tf.control_dependencies([saved_output.assign(output),\n",
    "                                saved_state.assign(state)]):\n",
    "    # Classifier.\n",
    "    logits = tf.nn.xw_plus_b(tf.concat(0, outputs), w, b)\n",
    "    loss = tf.reduce_mean(\n",
    "      tf.nn.softmax_cross_entropy_with_logits(\n",
    "        logits, tf.concat(0, train_labels)))\n",
    "\n",
    "  # Optimizer.\n",
    "  global_step = tf.Variable(0)\n",
    "  learning_rate = tf.train.exponential_decay(\n",
    "    10.0, global_step, 5000, 0.1, staircase=True)\n",
    "  optimizer = tf.train.GradientDescentOptimizer(learning_rate)\n",
    "  gradients, v = zip(*optimizer.compute_gradients(loss))\n",
    "  gradients, _ = tf.clip_by_global_norm(gradients, 1.25)\n",
    "  optimizer = optimizer.apply_gradients(\n",
    "    zip(gradients, v), global_step=global_step)\n",
    "\n",
    "  # Predictions.\n",
    "  train_prediction = tf.nn.softmax(logits)\n",
    "  \n",
    "  # Sampling and validation eval: batch 1, no unrolling.\n",
    "  sample_input = tf.placeholder(tf.float32, shape=[1, vocabulary_size])\n",
    "  saved_sample_output = tf.Variable(tf.zeros([1, num_nodes]))\n",
    "  saved_sample_state = tf.Variable(tf.zeros([1, num_nodes]))\n",
    "  reset_sample_state = tf.group(\n",
    "    saved_sample_output.assign(tf.zeros([1, num_nodes])),\n",
    "    saved_sample_state.assign(tf.zeros([1, num_nodes])))\n",
    "  sample_output, sample_state = lstm_cell(\n",
    "    sample_input, saved_sample_output, saved_sample_state)\n",
    "  with tf.control_dependencies([saved_sample_output.assign(sample_output),\n",
    "                                saved_sample_state.assign(sample_state)]):\n",
    "    sample_prediction = tf.nn.softmax(tf.nn.xw_plus_b(sample_output, w, b))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "cellView": "both",
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     },
     "output_extras": [
      {
       "item_id": 41
      },
      {
       "item_id": 80
      },
      {
       "item_id": 126
      },
      {
       "item_id": 144
      }
     ]
    },
    "colab_type": "code",
    "collapsed": false,
    "executionInfo": {
     "elapsed": 199909,
     "status": "ok",
     "timestamp": 1445965877333,
     "user": {
      "color": "#1FA15D",
      "displayName": "Vincent Vanhoucke",
      "isAnonymous": false,
      "isMe": true,
      "permissionId": "05076109866853157986",
      "photoUrl": "//lh6.googleusercontent.com/-cCJa7dTDcgQ/AAAAAAAAAAI/AAAAAAAACgw/r2EZ_8oYer4/s50-c-k-no/photo.jpg",
      "sessionId": "6f6f07b359200c46",
      "userId": "102167687554210253930"
     },
     "user_tz": 420
    },
    "id": "RD9zQCZTEaEm",
    "outputId": "5e868466-2532-4545-ce35-b403cf5d9de6"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initialized\n",
      "Average loss at step 0: 3.296854 learning rate: 10.000000\n",
      "Minibatch perplexity: 27.03\n",
      "================================================================================\n",
      "bg jhd npz ee pue vhotsffviikdi nnjjngs hibyezbcdatiukwefugrahihh tzlzemdimdefht\n",
      "tezgv cubpmwwddnrvicpcaept uvdbe cbmbzfmosigslsofmxmc vt boetx mtsqprfcyyoeelcnr\n",
      "ijtue jfgfhakootnphrbfapoeupn ei nvtouyzs elbeljpeazrghooiz e xdmiwuqiopn ipijad\n",
      "pfivencadnopceg tlaaesrzdab  oushumt neuy  er rce emvniuxifyxnhset jzaoi  iix sh\n",
      "nrtxaxe ani adoiatcllpfntirbhuay nefjintixccgiylteh  d k  bc lducvtqllyrsiheowch\n",
      "================================================================================\n",
      "Validation set perplexity: 20.30\n",
      "Average loss at step 100: 2.580736 learning rate: 10.000000\n",
      "Minibatch perplexity: 11.20\n",
      "Validation set perplexity: 10.74\n",
      "Average loss at step 200: 2.246526 learning rate: 10.000000\n",
      "Minibatch perplexity: 8.40\n",
      "Validation set perplexity: 8.60\n",
      "Average loss at step 300: 2.104012 learning rate: 10.000000\n",
      "Minibatch perplexity: 7.40\n",
      "Validation set perplexity: 8.04\n",
      "Average loss at step 400: 2.010656 learning rate: 10.000000\n",
      "Minibatch perplexity: 7.52\n",
      "Validation set perplexity: 7.78\n",
      "Average loss at step 500: 1.944645 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.66\n",
      "Validation set perplexity: 7.19\n",
      "Average loss at step 600: 1.917169 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.19\n",
      "Validation set perplexity: 7.15\n",
      "Average loss at step 700: 1.869214 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.52\n",
      "Validation set perplexity: 6.59\n",
      "Average loss at step 800: 1.827666 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.98\n",
      "Validation set perplexity: 6.42\n",
      "Average loss at step 900: 1.841973 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.77\n",
      "Validation set perplexity: 6.45\n",
      "Average loss at step 1000: 1.838061 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.81\n",
      "================================================================================\n",
      "l caincter is cludrihia strud poperate exame of age zer so mbrysoor of their one\n",
      "y farlite preskict wepr on novel regral as nanued press vully hamiea anta or a m\n",
      "xional vix seopen a penters was the moctem in efploce in hornist cenius to dosto\n",
      "fer in nine by in one of esiluding rove batten firgion of agad the film normibla\n",
      "ver of chution one zero energy of it bic four and of us he subels from the crom \n",
      "================================================================================\n",
      "Validation set perplexity: 6.18\n",
      "Average loss at step 1100: 1.784531 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.53\n",
      "Validation set perplexity: 6.07\n",
      "Average loss at step 1200: 1.761601 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.07\n",
      "Validation set perplexity: 5.81\n",
      "Average loss at step 1300: 1.738094 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.70\n",
      "Validation set perplexity: 5.76\n",
      "Average loss at step 1400: 1.752301 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.11\n",
      "Validation set perplexity: 5.58\n",
      "Average loss at step 1500: 1.741703 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.79\n",
      "Validation set perplexity: 5.65\n",
      "Average loss at step 1600: 1.748761 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.55\n",
      "Validation set perplexity: 5.59\n",
      "Average loss at step 1700: 1.713687 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.89\n",
      "Validation set perplexity: 5.45\n",
      "Average loss at step 1800: 1.679944 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.53\n",
      "Validation set perplexity: 5.44\n",
      "Average loss at step 1900: 1.649254 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.00\n",
      "Validation set perplexity: 5.39\n",
      "Average loss at step 2000: 1.702352 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.56\n",
      "================================================================================\n",
      "f production ditlin shork he glaning souse homide whe maiine in supturing noblin\n",
      "ess of united alvomances it knoody mevilican of the count and despecument prefis\n",
      "y sixted as consumble anoth and jelems norle protittignishires assown aspenked o\n",
      "vel mostre invigitory wend the stuply in the gior on one nine zero zerisserved w\n",
      "n pet prartists dons m internants croscari did progrount used of ninal jriguesla\n",
      "================================================================================\n",
      "Validation set perplexity: 5.20\n",
      "Average loss at step 2100: 1.687646 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.00\n",
      "Validation set perplexity: 5.05\n",
      "Average loss at step 2200: 1.682823 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.34\n",
      "Validation set perplexity: 5.04\n",
      "Average loss at step 2300: 1.646153 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.04\n",
      "Validation set perplexity: 4.99\n",
      "Average loss at step 2400: 1.664079 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.19\n",
      "Validation set perplexity: 4.80\n",
      "Average loss at step 2500: 1.683891 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.25\n",
      "Validation set perplexity: 4.77\n",
      "Average loss at step 2600: 1.654967 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.70\n",
      "Validation set perplexity: 4.69\n",
      "Average loss at step 2700: 1.659723 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.57\n",
      "Validation set perplexity: 4.76\n",
      "Average loss at step 2800: 1.652874 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.55\n",
      "Validation set perplexity: 4.66\n",
      "Average loss at step 2900: 1.652460 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.43\n",
      "Validation set perplexity: 4.73\n",
      "Average loss at step 3000: 1.654556 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.05\n",
      "================================================================================\n",
      "qually to kiby and as informatian used theij non ponged mace cols sommental betw\n",
      "vicate to descolove deleckinged be f n a pear karnicals peovonwist of contienian\n",
      "hed t tingalte than alpaite daart empion tradensaford chigh x and by one five mu\n",
      "x loygrally majaring inscorach partable a vection person sangengence bloy gordin\n",
      "ctantaling one zero zero s i is th as yeoricolly te papple sembij bocietihation \n",
      "================================================================================\n",
      "Validation set perplexity: 4.74\n",
      "Average loss at step 3100: 1.627729 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.79\n",
      "Validation set perplexity: 4.72\n",
      "Average loss at step 3200: 1.647606 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.55\n",
      "Validation set perplexity: 4.76\n",
      "Average loss at step 3300: 1.636961 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.89\n",
      "Validation set perplexity: 4.69\n",
      "Average loss at step 3400: 1.671301 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.57\n",
      "Validation set perplexity: 4.61\n",
      "Average loss at step 3500: 1.660412 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.53\n",
      "Validation set perplexity: 4.79\n",
      "Average loss at step 3600: 1.669594 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.50\n",
      "Validation set perplexity: 4.60\n",
      "Average loss at step 3700: 1.650854 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.09\n",
      "Validation set perplexity: 4.63\n",
      "Average loss at step 3800: 1.645876 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.45\n",
      "Validation set perplexity: 4.74\n",
      "Average loss at step 3900: 1.638574 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.29\n",
      "Validation set perplexity: 4.62\n",
      "Average loss at step 4000: 1.655002 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.78\n",
      "================================================================================\n",
      "ple aparage the unbe giam corvisiot is the himflation signs the destristion with\n",
      " were riphephance reposited in the jook are the eme peoplo postabason the from m\n",
      "g the sylt each they sciecces the hebake s styi one to fiele nota s sperord poin\n",
      "ed wrolera and jos orege at ded deparatamate in the epch is kelacomus who or hoi\n",
      "mill to heake of the ul samilate b s telemit isthic of laird sictowide the irmus\n",
      "================================================================================\n",
      "Validation set perplexity: 4.66\n",
      "Average loss at step 4100: 1.633853 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.26\n",
      "Validation set perplexity: 4.85\n",
      "Average loss at step 4200: 1.639595 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.18\n",
      "Validation set perplexity: 4.64\n",
      "Average loss at step 4300: 1.619418 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.99\n",
      "Validation set perplexity: 4.60\n",
      "Average loss at step 4400: 1.613333 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.89\n",
      "Validation set perplexity: 4.47\n",
      "Average loss at step 4500: 1.619956 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.22\n",
      "Validation set perplexity: 4.67\n",
      "Average loss at step 4600: 1.617407 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.01\n",
      "Validation set perplexity: 4.65\n",
      "Average loss at step 4700: 1.628697 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.21\n",
      "Validation set perplexity: 4.60\n",
      "Average loss at step 4800: 1.635972 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.41\n",
      "Validation set perplexity: 4.60\n",
      "Average loss at step 4900: 1.636446 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.15\n",
      "Validation set perplexity: 4.63\n",
      "Average loss at step 5000: 1.607716 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.49\n",
      "================================================================================\n",
      "zer of the her mainice of the enlegenal was in one nine five s linativel s poind\n",
      "per blecked in the resiidal a maken naturens inlisp one nine nine nine three one\n",
      "ine s froct that and the considery regation of a year k fastine the palining a t\n",
      "ent of the early thoom paulstracted is see kaves of like both general falling mo\n",
      "aching in one nine eia six sux at shat norlogy varo bluised l dengstia reportes \n",
      "================================================================================\n",
      "Validation set perplexity: 4.69\n",
      "Average loss at step 5100: 1.606979 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.03\n",
      "Validation set perplexity: 4.48\n",
      "Average loss at step 5200: 1.593128 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.51\n",
      "Validation set perplexity: 4.41\n",
      "Average loss at step 5300: 1.575173 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.48\n",
      "Validation set perplexity: 4.38\n",
      "Average loss at step 5400: 1.582943 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.11\n",
      "Validation set perplexity: 4.37\n",
      "Average loss at step 5500: 1.569006 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.77\n",
      "Validation set perplexity: 4.34\n",
      "Average loss at step 5600: 1.581801 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.76\n",
      "Validation set perplexity: 4.35\n",
      "Average loss at step 5700: 1.569846 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.45\n",
      "Validation set perplexity: 4.36\n",
      "Average loss at step 5800: 1.580088 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.89\n",
      "Validation set perplexity: 4.35\n",
      "Average loss at step 5900: 1.572867 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.11\n",
      "Validation set perplexity: 4.34\n",
      "Average loss at step 6000: 1.547184 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.00\n",
      "================================================================================\n",
      "niard uncomes in the deat in stotes excheption wiphable of two continuated by or\n",
      "leter and her was cradies native to but rayphol was holl a goin awarde usually o\n",
      "hi flature one staclure in new cantorich it parteen rulen the auth a passing dre\n",
      "hime d eight four one nine eight six three one nine six five y youghthhe reging \n",
      " reported a twang not other that chaton let se o sign with the takes southeru an\n",
      "================================================================================\n",
      "Validation set perplexity: 4.31\n",
      "Average loss at step 6100: 1.566445 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.17\n",
      "Validation set perplexity: 4.32\n",
      "Average loss at step 6200: 1.537641 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.89\n",
      "Validation set perplexity: 4.32\n",
      "Average loss at step 6300: 1.547715 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.83\n",
      "Validation set perplexity: 4.29\n",
      "Average loss at step 6400: 1.542947 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.44\n",
      "Validation set perplexity: 4.30\n",
      "Average loss at step 6500: 1.554286 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.62\n",
      "Validation set perplexity: 4.30\n",
      "Average loss at step 6600: 1.601062 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.82\n",
      "Validation set perplexity: 4.30\n",
      "Average loss at step 6700: 1.579278 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.05\n",
      "Validation set perplexity: 4.30\n",
      "Average loss at step 6800: 1.607842 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.60\n",
      "Validation set perplexity: 4.31\n",
      "Average loss at step 6900: 1.581312 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.63\n",
      "Validation set perplexity: 4.32\n",
      "Average loss at step 7000: 1.578187 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.25\n",
      "================================================================================\n",
      "x day to writer catermetroly a deam bay war oklought backering formants for stat\n",
      "blically wasturation some artivatore of exclume type value sunding are s later a\n",
      "ly tournuation of a rager a verw on six use john somera of babky duriterdian ove\n",
      "razime of economic by the surtrives of clarn and a chuical the kinon oave tronge\n",
      "le the six romes of article one zith time muppet seets belacted considerally acc\n",
      "================================================================================\n",
      "Validation set perplexity: 4.30\n"
     ]
    }
   ],
   "source": [
    "num_steps = 7001\n",
    "summary_frequency = 100\n",
    "\n",
    "with tf.Session(graph=graph) as session:\n",
    "  tf.initialize_all_variables().run()\n",
    "  print('Initialized')\n",
    "  mean_loss = 0\n",
    "  for step in range(num_steps):\n",
    "    batches = train_batches.next()\n",
    "    feed_dict = dict()\n",
    "    for i in range(num_unrollings + 1):\n",
    "      feed_dict[train_data[i]] = batches[i]\n",
    "    _, l, predictions, lr = session.run(\n",
    "      [optimizer, loss, train_prediction, learning_rate], feed_dict=feed_dict)\n",
    "    mean_loss += l\n",
    "    if step % summary_frequency == 0:\n",
    "      if step > 0:\n",
    "        mean_loss = mean_loss / summary_frequency\n",
    "      # The mean loss is an estimate of the loss over the last few batches.\n",
    "      print(\n",
    "        'Average loss at step %d: %f learning rate: %f' % (step, mean_loss, lr))\n",
    "      mean_loss = 0\n",
    "      labels = np.concatenate(list(batches)[1:])\n",
    "      print('Minibatch perplexity: %.2f' % float(\n",
    "        np.exp(logprob(predictions, labels))))\n",
    "      if step % (summary_frequency * 10) == 0:\n",
    "        # Generate some samples.\n",
    "        print('=' * 80)\n",
    "        for _ in range(5):\n",
    "          feed = sample(random_distribution())\n",
    "          sentence = characters(feed)[0]\n",
    "          reset_sample_state.run()\n",
    "          for _ in range(79):\n",
    "            prediction = sample_prediction.eval({sample_input: feed})\n",
    "            feed = sample(prediction)\n",
    "            sentence += characters(feed)[0]\n",
    "          print(sentence)\n",
    "        print('=' * 80)\n",
    "      # Measure validation set perplexity.\n",
    "      reset_sample_state.run()\n",
    "      valid_logprob = 0\n",
    "      for _ in range(valid_size):\n",
    "        b = valid_batches.next()\n",
    "        predictions = sample_prediction.eval({sample_input: b[0]})\n",
    "        valid_logprob = valid_logprob + logprob(predictions, b[1])\n",
    "      print('Validation set perplexity: %.2f' % float(np.exp(\n",
    "        valid_logprob / valid_size)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "pl4vtmFfa5nn"
   },
   "source": [
    "---\n",
    "Problem 1\n",
    "---------\n",
    "\n",
    "You might have noticed that the definition of the LSTM cell involves 4 matrix multiplications with the input, and 4 matrix multiplications with the output. Simplify the expression by using a single matrix multiply for each, and variables that are 4 times larger.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "num_nodes = 64\n",
    "\n",
    "graph = tf.Graph()\n",
    "with graph.as_default():\n",
    "  inMat = tf.Variable(tf.truncated_normal([vocabulary_size, 4*num_nodes], -0.1, 0.1))\n",
    "  outMat = tf.Variable(tf.truncated_normal([num_nodes, 4*num_nodes], -0.1, 0.1))\n",
    "  bias =  tf.Variable(tf.zeros([1,4*num_nodes])) \n",
    "  # Variables saving state across unrollings.\n",
    "  saved_output = tf.Variable(tf.zeros([batch_size, num_nodes]), trainable=False)\n",
    "  saved_state = tf.Variable(tf.zeros([batch_size, num_nodes]), trainable=False)\n",
    "  # Classifier weights and biases.\n",
    "  w = tf.Variable(tf.truncated_normal([num_nodes, vocabulary_size], -0.1, 0.1))\n",
    "  b = tf.Variable(tf.zeros([vocabulary_size]))\n",
    "  # Definition of the cell computation.\n",
    "  def lstm_cell(i, o, state):\n",
    "    \"\"\"Create a LSTM cell. See e.g.: http://arxiv.org/pdf/1402.1128v1.pdf\n",
    "    Note that in this formulation, we omit the various connections between the\n",
    "    previous state and the gates.\"\"\"\n",
    "    overallIn = tf.matmul(i, inMat)\n",
    "    #print(overallIn[0,0:10])\n",
    "    #print(overallIn[:,0:10])  \n",
    "    overallOut = tf.matmul(o, outMat)\n",
    "    input_gate = tf.sigmoid(overallIn[:,0:num_nodes] + \n",
    "                             overallOut[:,0:num_nodes] + bias[:,0:num_nodes])\n",
    "    forget_gate = tf.sigmoid(overallIn[:,num_nodes:2*num_nodes] + \n",
    "                             overallOut[:,num_nodes:2*num_nodes] + bias[:,num_nodes:2*num_nodes])\n",
    "    update = (overallIn[:,2*num_nodes:3*num_nodes] + \n",
    "                             overallOut[:,2*num_nodes:3*num_nodes] + bias[:,2*num_nodes:3*num_nodes])\n",
    "    state = forget_gate * state + input_gate * tf.tanh(update)\n",
    "    output_gate = tf.sigmoid(overallIn[:,3*num_nodes:4*num_nodes] + \n",
    "                             overallOut[:,3*num_nodes:4*num_nodes] + bias[:,3*num_nodes:4*num_nodes])\n",
    "    return output_gate * tf.tanh(state), state\n",
    "\n",
    "  # Input data.\n",
    "  train_data = list()\n",
    "  for _ in range(num_unrollings + 1):\n",
    "    train_data.append(\n",
    "      tf.placeholder(tf.float32, shape=[batch_size,vocabulary_size]))\n",
    "  train_inputs = train_data[:num_unrollings]\n",
    "  train_labels = train_data[1:]  # labels are inputs shifted by one time step.\n",
    "\n",
    "  # Unrolled LSTM loop.\n",
    "  outputs = list()\n",
    "  output = saved_output\n",
    "  state = saved_state\n",
    "  for i in train_inputs:\n",
    "    output, state = lstm_cell(i, output, state)\n",
    "    outputs.append(output)\n",
    "\n",
    "  # State saving across unrollings.\n",
    "  with tf.control_dependencies([saved_output.assign(output),\n",
    "                                saved_state.assign(state)]):\n",
    "    # Classifier.\n",
    "    logits = tf.nn.xw_plus_b(tf.concat(0, outputs), w, b)\n",
    "    loss = tf.reduce_mean(\n",
    "      tf.nn.softmax_cross_entropy_with_logits(\n",
    "        logits, tf.concat(0, train_labels)))\n",
    "\n",
    "  # Optimizer.\n",
    "  global_step = tf.Variable(0)\n",
    "  learning_rate = tf.train.exponential_decay(\n",
    "    10.0, global_step, 5000, 0.1, staircase=True)\n",
    "  optimizer = tf.train.GradientDescentOptimizer(learning_rate)\n",
    "  gradients, v = zip(*optimizer.compute_gradients(loss))\n",
    "  gradients, _ = tf.clip_by_global_norm(gradients, 1.25)\n",
    "  optimizer = optimizer.apply_gradients(\n",
    "    zip(gradients, v), global_step=global_step)\n",
    "\n",
    "  # Predictions.\n",
    "  train_prediction = tf.nn.softmax(logits)\n",
    "  \n",
    "  # Sampling and validation eval: batch 1, no unrolling.\n",
    "  sample_input = tf.placeholder(tf.float32, shape=[1, vocabulary_size])\n",
    "  saved_sample_output = tf.Variable(tf.zeros([1, num_nodes]))\n",
    "  saved_sample_state = tf.Variable(tf.zeros([1, num_nodes]))\n",
    "  reset_sample_state = tf.group(\n",
    "    saved_sample_output.assign(tf.zeros([1, num_nodes])),\n",
    "    saved_sample_state.assign(tf.zeros([1, num_nodes])))\n",
    "  sample_output, sample_state = lstm_cell(\n",
    "    sample_input, saved_sample_output, saved_sample_state)\n",
    "  with tf.control_dependencies([saved_sample_output.assign(sample_output),\n",
    "                                saved_sample_state.assign(sample_state)]):\n",
    "    sample_prediction = tf.nn.softmax(tf.nn.xw_plus_b(sample_output, w, b))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initialized\n",
      "Average loss at step 0: 3.294219 learning rate: 10.000000\n",
      "Minibatch perplexity: 26.96\n",
      "================================================================================\n",
      "vfyenaiprqljcdesm jz gwpcnjtmzad tmhoaldvhpisqme anfrcek tbjefhrijdatula hudehii\n",
      "djh ntgage zb  eghdt mmjjprtid  ovexlrlyktwwieo   elaqfn koexj tchiskfuyklkbcgh \n",
      "xu ckwmvy i p zptew jvnraoeitttkrvasatylnt wenhobvpcxh fd tiabm  mclh gjzu tkamm\n",
      "zm qasc f iw  retpluspaihivfr mu chtkv lqeare  ey z  wnlbq yu hfmfviptl uj agesl\n",
      "cwol as fxlu  thokjgyvvjjpbnedih d x lfxexhkwar  enab eek dfkhk  ncfamnra z e wm\n",
      "================================================================================\n",
      "Validation set perplexity: 20.16\n",
      "Average loss at step 100: 2.591002 learning rate: 10.000000\n",
      "Minibatch perplexity: 10.53\n",
      "Validation set perplexity: 10.07\n",
      "Average loss at step 200: 2.246096 learning rate: 10.000000\n",
      "Minibatch perplexity: 8.50\n",
      "Validation set perplexity: 8.47\n",
      "Average loss at step 300: 2.103330 learning rate: 10.000000\n",
      "Minibatch perplexity: 7.40\n",
      "Validation set perplexity: 8.08\n",
      "Average loss at step 400: 2.003574 learning rate: 10.000000\n",
      "Minibatch perplexity: 7.40\n",
      "Validation set perplexity: 7.73\n",
      "Average loss at step 500: 1.939874 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.43\n",
      "Validation set perplexity: 7.10\n",
      "Average loss at step 600: 1.913398 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.19\n",
      "Validation set perplexity: 6.81\n",
      "Average loss at step 700: 1.865266 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.39\n",
      "Validation set perplexity: 6.56\n",
      "Average loss at step 800: 1.824397 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.00\n",
      "Validation set perplexity: 6.44\n",
      "Average loss at step 900: 1.831897 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.93\n",
      "Validation set perplexity: 6.31\n",
      "Average loss at step 1000: 1.828882 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.64\n",
      "================================================================================\n",
      "libution disficioboral cenver of beer cold the sete from the hunded dewide hoo w\n",
      "ban perayicing to thise senine of endidions littol of are chnicug one reopee sla\n",
      "ze two five five four vinch sive he hy has dobuct in preofra the trressing of be\n",
      "vile solul hever becibn of the lenelly blea besmanabilf which uted one seven sev\n",
      "jest daiace the which hecess tover from elooulem co meiye refict being sconing i\n",
      "================================================================================\n",
      "Validation set perplexity: 6.11\n",
      "Average loss at step 1100: 1.784232 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.63\n",
      "Validation set perplexity: 5.89\n",
      "Average loss at step 1200: 1.757789 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.07\n",
      "Validation set perplexity: 5.62\n",
      "Average loss at step 1300: 1.738812 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.79\n",
      "Validation set perplexity: 5.63\n",
      "Average loss at step 1400: 1.748819 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.11\n",
      "Validation set perplexity: 5.55\n",
      "Average loss at step 1500: 1.740680 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.83\n",
      "Validation set perplexity: 5.43\n",
      "Average loss at step 1600: 1.747909 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.66\n",
      "Validation set perplexity: 5.45\n",
      "Average loss at step 1700: 1.716154 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.66\n",
      "Validation set perplexity: 5.36\n",
      "Average loss at step 1800: 1.677071 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.62\n",
      "Validation set perplexity: 5.24\n",
      "Average loss at step 1900: 1.646909 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.04\n",
      "Validation set perplexity: 5.22\n",
      "Average loss at step 2000: 1.698111 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.68\n",
      "================================================================================\n",
      "m legit in a kinchistical meagifices commaic opconured blangitely word pannt he \n",
      "rap same wheme a god than forspy puinch the diraiies it is the forned spitcint o\n",
      "j earon cas a rong moros of the somaspers antitally seven eight qu the manise a \n",
      "d often who partics bedoning greena d as the werework ire see liteving forthens \n",
      "bics outs expected un defance was showle efficiitive the the empirers be univers\n",
      "================================================================================\n",
      "Validation set perplexity: 5.20\n",
      "Average loss at step 2100: 1.688118 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.10\n",
      "Validation set perplexity: 4.91\n",
      "Average loss at step 2200: 1.681956 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.53\n",
      "Validation set perplexity: 5.07\n",
      "Average loss at step 2300: 1.641354 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.96\n",
      "Validation set perplexity: 4.93\n",
      "Average loss at step 2400: 1.665542 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.19\n",
      "Validation set perplexity: 4.80\n",
      "Average loss at step 2500: 1.682718 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.51\n",
      "Validation set perplexity: 4.73\n",
      "Average loss at step 2600: 1.658776 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.74\n",
      "Validation set perplexity: 4.67\n",
      "Average loss at step 2700: 1.659685 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.59\n",
      "Validation set perplexity: 4.72\n",
      "Average loss at step 2800: 1.653517 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.49\n",
      "Validation set perplexity: 4.65\n",
      "Average loss at step 2900: 1.654153 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.67\n",
      "Validation set perplexity: 4.64\n",
      "Average loss at step 3000: 1.652071 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.03\n",
      "================================================================================\n",
      "t vend become in the jade befourch the of prysensa and umeivation of steared bec\n",
      "ark of emperour hamiody as rearm of hold inslovent popuse his pannady all takes \n",
      "hdanms as they unisules ha doougost after pussion corninate indipially lival con\n",
      "histake the text in and moncypiple in frinch tran gearde and ambanu abs endrals \n",
      "k end that gamportin in a sansimation in alson or like and of soothment initist \n",
      "================================================================================\n",
      "Validation set perplexity: 4.63\n",
      "Average loss at step 3100: 1.631586 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.78\n",
      "Validation set perplexity: 4.65\n",
      "Average loss at step 3200: 1.647445 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.61\n",
      "Validation set perplexity: 4.60\n",
      "Average loss at step 3300: 1.637891 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.19\n",
      "Validation set perplexity: 4.58\n",
      "Average loss at step 3400: 1.667309 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.33\n",
      "Validation set perplexity: 4.57\n",
      "Average loss at step 3500: 1.656939 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.40\n",
      "Validation set perplexity: 4.66\n",
      "Average loss at step 3600: 1.671544 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.61\n",
      "Validation set perplexity: 4.55\n",
      "Average loss at step 3700: 1.648489 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.95\n",
      "Validation set perplexity: 4.53\n",
      "Average loss at step 3800: 1.647701 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.54\n",
      "Validation set perplexity: 4.68\n",
      "Average loss at step 3900: 1.641488 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.19\n",
      "Validation set perplexity: 4.57\n",
      "Average loss at step 4000: 1.654163 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.82\n",
      "================================================================================\n",
      "ans and pupputes she wand and play balear one one poally geornogo or gencont zea\n",
      "ble where american crise beland d and on or allowetegation apperrente opcesic ye\n",
      "what not he frenth anthratical uscessies kene a a not viet with homporso poperof\n",
      "ch for milital erstemed geries an eastrany in view the batteq of the ber ugus ra\n",
      "que was befams a dapper socrap bit origin the may claims iv marroy defores worad\n",
      "================================================================================\n",
      "Validation set perplexity: 4.49\n",
      "Average loss at step 4100: 1.633809 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.19\n",
      "Validation set perplexity: 4.63\n",
      "Average loss at step 4200: 1.638370 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.41\n",
      "Validation set perplexity: 4.51\n",
      "Average loss at step 4300: 1.616586 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.11\n",
      "Validation set perplexity: 4.49\n",
      "Average loss at step 4400: 1.613784 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.69\n",
      "Validation set perplexity: 4.40\n",
      "Average loss at step 4500: 1.616972 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.27\n",
      "Validation set perplexity: 4.58\n",
      "Average loss at step 4600: 1.614950 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.76\n",
      "Validation set perplexity: 4.62\n",
      "Average loss at step 4700: 1.626204 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.18\n",
      "Validation set perplexity: 4.37\n",
      "Average loss at step 4800: 1.629704 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.40\n",
      "Validation set perplexity: 4.40\n",
      "Average loss at step 4900: 1.632723 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.20\n",
      "Validation set perplexity: 4.49\n",
      "Average loss at step 5000: 1.610236 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.59\n",
      "================================================================================\n",
      "hy note s mostwopke sanse thmous legiated and descrazining in his it is an liste\n",
      "mens of lorgen was at cassuld ismak in of to dra enorg binks one nine zero zero \n",
      "ne meshie which a porson bjoted as the replared of belaw mach ponatione suchable\n",
      "x adi c willian permem work attensel semilast his frank to the gacing the worlde\n",
      "ging other his by that spitual could ks criged as probitienk there american ootr\n",
      "================================================================================\n",
      "Validation set perplexity: 4.63\n",
      "Average loss at step 5100: 1.606547 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.02\n",
      "Validation set perplexity: 4.40\n",
      "Average loss at step 5200: 1.592767 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.57\n",
      "Validation set perplexity: 4.35\n",
      "Average loss at step 5300: 1.577739 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.62\n",
      "Validation set perplexity: 4.32\n",
      "Average loss at step 5400: 1.578766 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.12\n",
      "Validation set perplexity: 4.30\n",
      "Average loss at step 5500: 1.568728 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.02\n",
      "Validation set perplexity: 4.27\n",
      "Average loss at step 5600: 1.577826 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.95\n",
      "Validation set perplexity: 4.26\n",
      "Average loss at step 5700: 1.571620 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.53\n",
      "Validation set perplexity: 4.28\n",
      "Average loss at step 5800: 1.582991 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.87\n",
      "Validation set perplexity: 4.28\n",
      "Average loss at step 5900: 1.574133 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.07\n",
      "Validation set perplexity: 4.27\n",
      "Average loss at step 6000: 1.550386 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.22\n",
      "================================================================================\n",
      "s autom and hour schox original ined a latindt with monees and selle years world\n",
      "bartary i ismause of the set two zero zero three one nine four eight eight zero \n",
      "k continus the take famaties of a subjents all is and of dominatior wor can of a\n",
      "ques take of the bomer that colleme segine isvaser of the most and to execulal m\n",
      "regha will paliok was they and she againsting four st aberican to retuptory tuil\n",
      "================================================================================\n",
      "Validation set perplexity: 4.27\n",
      "Average loss at step 6100: 1.564372 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.90\n",
      "Validation set perplexity: 4.22\n",
      "Average loss at step 6200: 1.536422 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.74\n",
      "Validation set perplexity: 4.24\n",
      "Average loss at step 6300: 1.544394 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.94\n",
      "Validation set perplexity: 4.24\n",
      "Average loss at step 6400: 1.540413 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.57\n",
      "Validation set perplexity: 4.25\n",
      "Average loss at step 6500: 1.554786 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.66\n",
      "Validation set perplexity: 4.24\n",
      "Average loss at step 6600: 1.591832 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.84\n",
      "Validation set perplexity: 4.21\n",
      "Average loss at step 6700: 1.581067 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.01\n",
      "Validation set perplexity: 4.24\n",
      "Average loss at step 6800: 1.605958 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.76\n",
      "Validation set perplexity: 4.22\n",
      "Average loss at step 6900: 1.585254 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.71\n",
      "Validation set perplexity: 4.24\n",
      "Average loss at step 7000: 1.577838 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.05\n",
      "================================================================================\n",
      "ly the exist timple originus not that this a canlew by man cootempt a baw defanc\n",
      "n is de formatved infrucence toulth procextion it of no mogation guhn refer the \n",
      "lap swewsomiss and numbers to s and by repursical disuase crite europe diseace l\n",
      "ing andurer hawn considement of the remming to only as nif trac one eight seven \n",
      "ried prognics area now wapare with the most increase and backer its fello pairs \n",
      "================================================================================\n",
      "Validation set perplexity: 4.22\n"
     ]
    }
   ],
   "source": [
    "num_steps = 7001\n",
    "summary_frequency = 100\n",
    "\n",
    "with tf.Session(graph=graph) as session:\n",
    "  tf.initialize_all_variables().run()\n",
    "  print('Initialized')\n",
    "  mean_loss = 0\n",
    "  for step in range(num_steps):\n",
    "    batches = train_batches.next()\n",
    "    feed_dict = dict()\n",
    "    for i in range(num_unrollings + 1):\n",
    "      feed_dict[train_data[i]] = batches[i]\n",
    "    _, l, predictions, lr = session.run(\n",
    "      [optimizer, loss, train_prediction, learning_rate], feed_dict=feed_dict)\n",
    "    mean_loss += l\n",
    "    if step % summary_frequency == 0:\n",
    "      if step > 0:\n",
    "        mean_loss = mean_loss / summary_frequency\n",
    "      # The mean loss is an estimate of the loss over the last few batches.\n",
    "      print(\n",
    "        'Average loss at step %d: %f learning rate: %f' % (step, mean_loss, lr))\n",
    "      mean_loss = 0\n",
    "      labels = np.concatenate(list(batches)[1:])\n",
    "      print('Minibatch perplexity: %.2f' % float(\n",
    "        np.exp(logprob(predictions, labels))))\n",
    "      if step % (summary_frequency * 10) == 0:\n",
    "        # Generate some samples.\n",
    "        print('=' * 80)\n",
    "        for _ in range(5):\n",
    "          feed = sample(random_distribution())\n",
    "          sentence = characters(feed)[0]\n",
    "          reset_sample_state.run()\n",
    "          for _ in range(79):\n",
    "            prediction = sample_prediction.eval({sample_input: feed})\n",
    "            feed = sample(prediction)\n",
    "            sentence += characters(feed)[0]\n",
    "          print(sentence)\n",
    "        print('=' * 80)\n",
    "      # Measure validation set perplexity.\n",
    "      reset_sample_state.run()\n",
    "      valid_logprob = 0\n",
    "      for _ in range(valid_size):\n",
    "        b = valid_batches.next()\n",
    "        predictions = sample_prediction.eval({sample_input: b[0]})\n",
    "        valid_logprob = valid_logprob + logprob(predictions, b[1])\n",
    "      print('Validation set perplexity: %.2f' % float(np.exp(\n",
    "        valid_logprob / valid_size)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "4eErTCTybtph"
   },
   "source": [
    "---\n",
    "Problem 2\n",
    "---------\n",
    "\n",
    "We want to train a LSTM over bigrams, that is pairs of consecutive characters like 'ab' instead of single characters like 'a'. Since the number of possible bigrams is large, feeding them directly to the LSTM using 1-hot encodings will lead to a very sparse representation that is very wasteful computationally.\n",
    "\n",
    "a- Introduce an embedding lookup on the inputs, and feed the embeddings to the LSTM cell instead of the inputs themselves.\n",
    "\n",
    "b- Write a bigram-based LSTM, modeled on the character LSTM above.\n",
    "\n",
    "c- Introduce Dropout. For best practices on how to use Dropout in LSTMs, refer to this [article](http://arxiv.org/abs/1409.2329).\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "num_nodes = 256\n",
    "# construct bigram by using embedding for each character.\n",
    "embed_dim = 32\n",
    "\n",
    "batch_size=64\n",
    "num_unrollings=10\n",
    "kp = 1.0\n",
    "train_batches = BatchGenerator(train_text, batch_size, num_unrollings)\n",
    "valid_batches = BatchGenerator(valid_text, 1, 2)\n",
    "\n",
    "graph = tf.Graph()\n",
    "with graph.as_default():\n",
    "  embedMat = tf.Variable(\n",
    "    tf.random_uniform([vocabulary_size*vocabulary_size, embed_dim], -1.0, 1.0))\n",
    "  inMat = tf.Variable(tf.truncated_normal([embed_dim, 4*num_nodes], -0.1, 0.1))\n",
    "  outMat = tf.Variable(tf.truncated_normal([num_nodes, 4*num_nodes], -0.1, 0.1))\n",
    "  bias =  tf.Variable(tf.zeros([1,4*num_nodes])) \n",
    "  keep_prob = tf.placeholder(\"float\")\n",
    "  # Variables saving state across unrollings.\n",
    "  saved_output = tf.Variable(tf.zeros([batch_size, num_nodes]), trainable=False)\n",
    "  saved_state = tf.Variable(tf.zeros([batch_size, num_nodes]), trainable=False)\n",
    "  # Classifier weights and biases.\n",
    "  w = tf.Variable(tf.truncated_normal([num_nodes, vocabulary_size], -0.1, 0.1))\n",
    "  b = tf.Variable(tf.zeros([vocabulary_size]))\n",
    "  # Definition of the cell computation.\n",
    "  def lstm_cell(i, o, state):\n",
    "    \"\"\"Create a LSTM cell. See e.g.: http://arxiv.org/pdf/1402.1128v1.pdf\n",
    "    Note that in this formulation, we omit the various connections between the\n",
    "    previous state and the gates.\"\"\"\n",
    "    overallIn = tf.nn.dropout(tf.matmul(i, inMat),keep_prob)\n",
    "    #print(overallIn[0,0:10])\n",
    "    #print(overallIn[:,0:10])  \n",
    "    overallOut = tf.matmul(o, outMat)\n",
    "    input_gate = tf.sigmoid(overallIn[:,0:num_nodes] + \n",
    "                             overallOut[:,0:num_nodes] + bias[:,0:num_nodes])\n",
    "    forget_gate = tf.sigmoid(overallIn[:,num_nodes:2*num_nodes] + \n",
    "                             overallOut[:,num_nodes:2*num_nodes] + bias[:,num_nodes:2*num_nodes])\n",
    "    update = (overallIn[:,2*num_nodes:3*num_nodes] + \n",
    "                             overallOut[:,2*num_nodes:3*num_nodes] + bias[:,2*num_nodes:3*num_nodes])\n",
    "    state = forget_gate * state + input_gate * tf.tanh(update)\n",
    "    output_gate = tf.sigmoid(overallIn[:,3*num_nodes:4*num_nodes] + \n",
    "                             overallOut[:,3*num_nodes:4*num_nodes] + bias[:,3*num_nodes:4*num_nodes])\n",
    "    return tf.nn.dropout(output_gate * tf.tanh(state),keep_prob), state\n",
    "\n",
    "  # Input data.\n",
    "  train_inputs = list()\n",
    "  train_labels = list()\n",
    "  for _ in range(num_unrollings-1):\n",
    "    train_inputs.append(\n",
    "      tf.placeholder(tf.int32, shape=[batch_size]))\n",
    "  for _ in range(num_unrollings-1):\n",
    "    train_labels.append(\n",
    "      tf.placeholder(tf.float32, shape=[batch_size,vocabulary_size]))  \n",
    "    \n",
    "  # Unrolled LSTM loop.\n",
    "  outputs = list()\n",
    "  output = saved_output\n",
    "  state = saved_state\n",
    "  for i in range(num_unrollings-1):\n",
    "    in1 = tf.nn.embedding_lookup(embedMat, train_inputs[i])\n",
    "    #print(tf.shape(in1))\n",
    "    #in1 = tf.matmul(train_inputs[i], embedMat) + tf.matmul(train_inputs[i+1], embedMat)\n",
    "    output, state = lstm_cell(in1, output, state)\n",
    "    outputs.append(output)\n",
    "\n",
    "  # State saving across unrollings.\n",
    "  with tf.control_dependencies([saved_output.assign(output),\n",
    "                                saved_state.assign(state)]):\n",
    "    # Classifier.\n",
    "    logits = tf.nn.xw_plus_b(tf.concat(0, outputs), w, b)\n",
    "    loss = tf.reduce_mean(\n",
    "      tf.nn.softmax_cross_entropy_with_logits(\n",
    "        logits, tf.concat(0, train_labels)))\n",
    "\n",
    "  # Optimizer.\n",
    "  global_step = tf.Variable(0)\n",
    "  learning_rate = tf.train.exponential_decay(\n",
    "    20.0, global_step, 10000, 0.5, staircase=True)\n",
    "  optimizer = tf.train.GradientDescentOptimizer(learning_rate)\n",
    "  gradients, v = zip(*optimizer.compute_gradients(loss))\n",
    "  gradients, _ = tf.clip_by_global_norm(gradients, 1.25)\n",
    "  optimizer = optimizer.apply_gradients(\n",
    "    zip(gradients, v), global_step=global_step)\n",
    "\n",
    "  # Predictions.\n",
    "  train_prediction = tf.nn.softmax(logits)\n",
    "  \n",
    "  # Sampling and validation eval: batch 1, no unrolling.\n",
    "  sample_input_1 = tf.placeholder(tf.int32, shape=[1])\n",
    "  sample_in = tf.nn.embedding_lookup(embedMat, sample_input_1)\n",
    "  #sample_in = tf.matmul(sample_input_1, embedMat)+tf.matmul(sample_input_2, embedMat)  \n",
    "  saved_sample_output = tf.Variable(tf.zeros([1, num_nodes]))\n",
    "  saved_sample_state = tf.Variable(tf.zeros([1, num_nodes]))\n",
    "  reset_sample_state = tf.group(\n",
    "    saved_sample_output.assign(tf.zeros([1, num_nodes])),\n",
    "    saved_sample_state.assign(tf.zeros([1, num_nodes])))\n",
    "  sample_output, sample_state = lstm_cell(\n",
    "    sample_in, saved_sample_output, saved_sample_state)\n",
    "  with tf.control_dependencies([saved_sample_output.assign(sample_output),\n",
    "                                saved_sample_state.assign(sample_state)]):\n",
    "    sample_prediction = tf.nn.softmax(tf.nn.xw_plus_b(sample_output, w, b))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def convertOneHot(b):\n",
    "  nr,nc = b.shape\n",
    "  o = np.zeros(shape = [nr],dtype = int)\n",
    "  for i in range(nr):\n",
    "    o[i] = np.where(b[i,:]>0.1)[0]\n",
    "  return o"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initialized\n",
      "Average loss at step 0: 3.514919 learning rate: 20.000000\n",
      "Minibatch perplexity: 33.61\n",
      "================================================================================\n",
      "vc p b a a  w b n e  q  b vq e rs rg e t hk qp ru e f e s  j  k cb  k s qa  b  g \n",
      "fr p  y a  r m y u e  j  g  p p  t s vq  w  p j  za  h cn   k a qw dr e oj n s  m\n",
      " j   q e  r  o k g rt  w  m  z p i t e  r e rf e  x jur b l q  a f xi  k e  t dj \n",
      "ank e tna e  h a i vy  d t zt  q n t a  j  h zq n y  v t zi vd  c ot  z  b e  z  \n",
      "fau p e dj ok  m  g n a  k t s e n  z  w  h f  j k b os p i  q l g  m n ql  v wf \n",
      "================================================================================\n",
      "Validation set perplexity: 5149.99\n",
      "Average loss at step 100: 3.842768 learning rate: 20.000000\n",
      "Minibatch perplexity: 16.86\n",
      "Validation set perplexity: 19.51\n",
      "Average loss at step 200: 2.671853 learning rate: 20.000000\n",
      "Minibatch perplexity: 12.31\n",
      "Validation set perplexity: 18.09\n",
      "Average loss at step 300: 2.557041 learning rate: 20.000000\n",
      "Minibatch perplexity: 11.53\n",
      "Validation set perplexity: 14.12\n",
      "Average loss at step 400: 2.474616 learning rate: 20.000000\n",
      "Minibatch perplexity: 10.98\n",
      "Validation set perplexity: 13.89\n",
      "Average loss at step 500: 2.419309 learning rate: 20.000000\n",
      "Minibatch perplexity: 10.40\n",
      "Validation set perplexity: 12.80\n",
      "Average loss at step 600: 2.358472 learning rate: 20.000000\n",
      "Minibatch perplexity: 10.50\n",
      "Validation set perplexity: 12.90\n",
      "Average loss at step 700: 2.318550 learning rate: 20.000000\n",
      "Minibatch perplexity: 10.24\n",
      "Validation set perplexity: 13.06\n",
      "Average loss at step 800: 2.314942 learning rate: 20.000000\n",
      "Minibatch perplexity: 10.34\n",
      "Validation set perplexity: 11.03\n",
      "Average loss at step 900: 2.240520 learning rate: 20.000000\n",
      "Minibatch perplexity: 8.75\n",
      "Validation set perplexity: 9.84\n",
      "Average loss at step 1000: 2.232397 learning rate: 20.000000\n",
      "Minibatch perplexity: 9.56\n",
      "================================================================================\n",
      " s peral tnhh isparly explation the of to ress from of lametern sh ar a mymes and\n",
      "wzero zero nine four in ish the whis the remamer was bu the stxampiyeas dayormomp\n",
      "ddoad laut btaliso sabertdproduc the expirs expirs severs ber mous four and bewcd\n",
      "cur of one essisight cal mild namating milissdne four reat the mosgamerne the fou\n",
      "xperfor and a samused the the lageraleasa marge dure statii exptudo zero for it b\n",
      "================================================================================\n",
      "Validation set perplexity: 9.08\n",
      "Average loss at step 1100: 2.237679 learning rate: 20.000000\n",
      "Minibatch perplexity: 9.16\n",
      "Validation set perplexity: 9.48\n",
      "Average loss at step 1200: 2.243504 learning rate: 20.000000\n",
      "Minibatch perplexity: 9.45\n",
      "Validation set perplexity: 9.73\n",
      "Average loss at step 1300: 2.202898 learning rate: 20.000000\n",
      "Minibatch perplexity: 9.99\n",
      "Validation set perplexity: 9.44\n",
      "Average loss at step 1400: 2.193813 learning rate: 20.000000\n",
      "Minibatch perplexity: 8.47\n",
      "Validation set perplexity: 9.06\n",
      "Average loss at step 1500: 2.197909 learning rate: 20.000000\n",
      "Minibatch perplexity: 8.62\n",
      "Validation set perplexity: 8.79\n",
      "Average loss at step 1600: 2.162536 learning rate: 20.000000\n",
      "Minibatch perplexity: 9.62\n",
      "Validation set perplexity: 9.19\n",
      "Average loss at step 1700: 2.170903 learning rate: 20.000000\n",
      "Minibatch perplexity: 8.35\n",
      "Validation set perplexity: 8.97\n",
      "Average loss at step 1800: 2.163664 learning rate: 20.000000\n",
      "Minibatch perplexity: 9.36\n",
      "Validation set perplexity: 8.69\n",
      "Average loss at step 1900: 2.201730 learning rate: 20.000000\n",
      "Minibatch perplexity: 8.35\n",
      "Validation set perplexity: 9.48\n",
      "Average loss at step 2000: 2.157675 learning rate: 20.000000\n",
      "Minibatch perplexity: 8.07\n",
      "================================================================================\n",
      "yfc amed canshat megetend chodecurhe expregard the holle bey the in of legaverpwg\n",
      "fbe langued chaventepty and he sevent the segangued the conse the premphisylated \n",
      "led whicas the matuten sevence nine ching the rempxamed feld sented two felle was\n",
      " munes clearth beed the exted the bektlent havess have eschive s of the hany beop\n",
      "cpulnsted the that in in s nine wele unds the be recesed neas was be in a we and \n",
      "================================================================================\n",
      "Validation set perplexity: 9.74\n",
      "Average loss at step 2100: 2.175937 learning rate: 20.000000\n",
      "Minibatch perplexity: 8.81\n",
      "Validation set perplexity: 8.92\n",
      "Average loss at step 2200: 2.130236 learning rate: 20.000000\n",
      "Minibatch perplexity: 9.17\n",
      "Validation set perplexity: 8.72\n",
      "Average loss at step 2300: 2.142528 learning rate: 20.000000\n",
      "Minibatch perplexity: 7.15\n",
      "Validation set perplexity: 8.79\n",
      "Average loss at step 2400: 2.139556 learning rate: 20.000000\n",
      "Minibatch perplexity: 7.85\n",
      "Validation set perplexity: 8.51\n",
      "Average loss at step 2500: 2.125112 learning rate: 20.000000\n",
      "Minibatch perplexity: 8.65\n",
      "Validation set perplexity: 8.87\n",
      "Average loss at step 2600: 2.123464 learning rate: 20.000000\n",
      "Minibatch perplexity: 7.94\n",
      "Validation set perplexity: 8.70\n",
      "Average loss at step 2700: 2.117308 learning rate: 20.000000\n",
      "Minibatch perplexity: 8.47\n",
      "Validation set perplexity: 8.35\n",
      "Average loss at step 2800: 2.129825 learning rate: 20.000000\n",
      "Minibatch perplexity: 8.38\n",
      "Validation set perplexity: 8.71\n",
      "Average loss at step 2900: 2.107768 learning rate: 20.000000\n",
      "Minibatch perplexity: 7.22\n",
      "Validation set perplexity: 8.14\n",
      "Average loss at step 3000: 2.118235 learning rate: 20.000000\n",
      "Minibatch perplexity: 8.40\n",
      "================================================================================\n",
      "ld art and whis about volized cour two und age sibn action a cod a and hava mak b\n",
      "oject of three fine caused requars of carmy gelase throught five recontrabrage ba\n",
      "stgs of as of the s of f the for of feigion in have have a rach cal for for four \n",
      "qkill fach phobing experammage quohning the agave hunne audof stath meigzamerance\n",
      "tj the grant a and int in thil fecarchis a came of jammunia the into a commics ba\n",
      "================================================================================\n",
      "Validation set perplexity: 8.57\n",
      "Average loss at step 3100: 2.091247 learning rate: 20.000000\n",
      "Minibatch perplexity: 7.42\n",
      "Validation set perplexity: 8.60\n",
      "Average loss at step 3200: 2.084515 learning rate: 20.000000\n",
      "Minibatch perplexity: 7.37\n",
      "Validation set perplexity: 8.37\n",
      "Average loss at step 3300: 2.106600 learning rate: 20.000000\n",
      "Minibatch perplexity: 8.90\n",
      "Validation set perplexity: 8.34\n",
      "Average loss at step 3400: 2.080199 learning rate: 20.000000\n",
      "Minibatch perplexity: 8.09\n",
      "Validation set perplexity: 8.79\n",
      "Average loss at step 3500: 2.073199 learning rate: 20.000000\n",
      "Minibatch perplexity: 8.02\n",
      "Validation set perplexity: 8.45\n",
      "Average loss at step 3600: 2.101094 learning rate: 20.000000\n",
      "Minibatch perplexity: 7.60\n",
      "Validation set perplexity: 8.26\n",
      "Average loss at step 3700: 2.070852 learning rate: 20.000000\n",
      "Minibatch perplexity: 7.65\n",
      "Validation set perplexity: 8.10\n",
      "Average loss at step 3800: 2.045473 learning rate: 20.000000\n",
      "Minibatch perplexity: 8.59\n",
      "Validation set perplexity: 8.44\n",
      "Average loss at step 3900: 2.077137 learning rate: 20.000000\n",
      "Minibatch perplexity: 9.10\n",
      "Validation set perplexity: 8.93\n",
      "Average loss at step 4000: 2.073441 learning rate: 20.000000\n",
      "Minibatch perplexity: 8.15\n",
      "================================================================================\n",
      " exterroliziled are by and broke viden complenty dom ber likision was the y of ba\n",
      "tp moriverscappland abriyat nine nine likles phing the zero zero five bes morged \n",
      "wthe calrions hish stany mal fales nine three rejohny ba from the make new conts \n",
      "phalnictions was from fuls be was lestal fe nolologrademels the by and all was co\n",
      "lgive the of properre for language leaing one charight in stations provically pri\n",
      "================================================================================\n",
      "Validation set perplexity: 7.73\n",
      "Average loss at step 4100: 2.083509 learning rate: 20.000000\n",
      "Minibatch perplexity: 9.41\n",
      "Validation set perplexity: 8.45\n",
      "Average loss at step 4200: 2.062570 learning rate: 20.000000\n",
      "Minibatch perplexity: 7.72\n",
      "Validation set perplexity: 8.19\n",
      "Average loss at step 4300: 2.057982 learning rate: 20.000000\n",
      "Minibatch perplexity: 8.75\n",
      "Validation set perplexity: 7.84\n",
      "Average loss at step 4400: 2.056195 learning rate: 20.000000\n",
      "Minibatch perplexity: 7.90\n",
      "Validation set perplexity: 7.99\n",
      "Average loss at step 4500: 2.097648 learning rate: 20.000000\n",
      "Minibatch perplexity: 7.43\n",
      "Validation set perplexity: 8.32\n",
      "Average loss at step 4600: 2.060939 learning rate: 20.000000\n",
      "Minibatch perplexity: 7.24\n",
      "Validation set perplexity: 8.11\n",
      "Average loss at step 4700: 2.043968 learning rate: 20.000000\n",
      "Minibatch perplexity: 7.13\n",
      "Validation set perplexity: 8.10\n",
      "Average loss at step 4800: 2.044852 learning rate: 20.000000\n",
      "Minibatch perplexity: 6.88\n",
      "Validation set perplexity: 8.00\n",
      "Average loss at step 4900: 2.002850 learning rate: 20.000000\n",
      "Minibatch perplexity: 7.71\n",
      "Validation set perplexity: 7.56\n",
      "Average loss at step 5000: 2.046728 learning rate: 20.000000\n",
      "Minibatch perplexity: 8.67\n",
      "================================================================================\n",
      "wjtark knowv forment was hand one sampliehrused and six major chat one one nine n\n",
      "vs the man by parth that formard to whill wormaut of of which a sorna licds mably\n",
      "ost one nine six five groubly three s four sock bastanive in hory of as cist that\n",
      "mr the vart porewith of six nine six eight nine one nine four five to cand cation\n",
      "heralamays and one nine seven in by surhusand working in and that s supfa in pris\n",
      "================================================================================\n",
      "Validation set perplexity: 8.05\n",
      "Average loss at step 5100: 2.056435 learning rate: 20.000000\n",
      "Minibatch perplexity: 9.13\n",
      "Validation set perplexity: 7.70\n",
      "Average loss at step 5200: 2.050836 learning rate: 20.000000\n",
      "Minibatch perplexity: 7.80\n",
      "Validation set perplexity: 7.51\n",
      "Average loss at step 5300: 2.049254 learning rate: 20.000000\n",
      "Minibatch perplexity: 6.73\n",
      "Validation set perplexity: 7.78\n",
      "Average loss at step 5400: 2.044946 learning rate: 20.000000\n",
      "Minibatch perplexity: 7.12\n",
      "Validation set perplexity: 8.01\n",
      "Average loss at step 5500: 2.021138 learning rate: 20.000000\n",
      "Minibatch perplexity: 7.25\n",
      "Validation set perplexity: 7.34\n",
      "Average loss at step 5600: 2.016377 learning rate: 20.000000\n",
      "Minibatch perplexity: 7.20\n",
      "Validation set perplexity: 7.47\n",
      "Average loss at step 5700: 2.032872 learning rate: 20.000000\n",
      "Minibatch perplexity: 7.58\n",
      "Validation set perplexity: 7.23\n",
      "Average loss at step 5800: 2.043068 learning rate: 20.000000\n",
      "Minibatch perplexity: 7.96\n",
      "Validation set perplexity: 7.32\n",
      "Average loss at step 5900: 2.040755 learning rate: 20.000000\n",
      "Minibatch perplexity: 6.83\n",
      "Validation set perplexity: 7.69\n",
      "Average loss at step 6000: 2.037628 learning rate: 20.000000\n",
      "Minibatch perplexity: 8.31\n",
      "================================================================================\n",
      "bjecked williked have is to the japte a kers of ning recientage repg to he nemer \n",
      "mties seven of the a for resericed incing beighted to is logerrence sare seelent \n",
      "tqsed seven mary one six meagohnitre from funical pe everachiten that were of the\n",
      "yring and e number judescujudian one seven eight three member communiation the th\n",
      "aabeoppolowere as caland ded are one worker of was haves as noto in the iss indev\n",
      "================================================================================\n",
      "Validation set perplexity: 7.19\n",
      "Average loss at step 6100: 2.016519 learning rate: 20.000000\n",
      "Minibatch perplexity: 7.05\n",
      "Validation set perplexity: 7.77\n",
      "Average loss at step 6200: 2.010100 learning rate: 20.000000\n",
      "Minibatch perplexity: 6.97\n",
      "Validation set perplexity: 7.19\n",
      "Average loss at step 6300: 2.035717 learning rate: 20.000000\n",
      "Minibatch perplexity: 8.28\n",
      "Validation set perplexity: 7.53\n",
      "Average loss at step 6400: 2.013948 learning rate: 20.000000\n",
      "Minibatch perplexity: 7.10\n",
      "Validation set perplexity: 7.13\n",
      "Average loss at step 6500: 2.025846 learning rate: 20.000000\n",
      "Minibatch perplexity: 7.74\n",
      "Validation set perplexity: 7.33\n",
      "Average loss at step 6600: 2.025806 learning rate: 20.000000\n",
      "Minibatch perplexity: 9.27\n",
      "Validation set perplexity: 7.41\n",
      "Average loss at step 6700: 2.019889 learning rate: 20.000000\n",
      "Minibatch perplexity: 8.42\n",
      "Validation set perplexity: 7.34\n",
      "Average loss at step 6800: 2.013713 learning rate: 20.000000\n",
      "Minibatch perplexity: 7.59\n",
      "Validation set perplexity: 7.42\n",
      "Average loss at step 6900: 1.992941 learning rate: 20.000000\n",
      "Minibatch perplexity: 7.98\n",
      "Validation set perplexity: 7.85\n",
      "Average loss at step 7000: 1.980383 learning rate: 20.000000\n",
      "Minibatch perplexity: 6.46\n",
      "================================================================================\n",
      "uon by the reconterect the madation was for midred nottere hough which beasity pr\n",
      "gme prede recevery danter of the etation one eight gengroup surpost alivers land \n",
      "qpopist one eight seven where al were of jachae deverice relowere the of his ben \n",
      "nter nore the unies mend the leen ditelly gooker the the of the three the recolle\n",
      "rjortwo zero four his one six three three two zero zero four jear in seven that v\n",
      "================================================================================\n",
      "Validation set perplexity: 7.56\n",
      "Average loss at step 7100: 1.998033 learning rate: 20.000000\n",
      "Minibatch perplexity: 7.13\n",
      "Validation set perplexity: 7.71\n",
      "Average loss at step 7200: 2.025653 learning rate: 20.000000\n",
      "Minibatch perplexity: 8.09\n",
      "Validation set perplexity: 7.47\n",
      "Average loss at step 7300: 1.994624 learning rate: 20.000000\n",
      "Minibatch perplexity: 7.51\n",
      "Validation set perplexity: 7.39\n",
      "Average loss at step 7400: 1.992129 learning rate: 20.000000\n",
      "Minibatch perplexity: 6.60\n",
      "Validation set perplexity: 7.11\n",
      "Average loss at step 7500: 2.008375 learning rate: 20.000000\n",
      "Minibatch perplexity: 7.32\n",
      "Validation set perplexity: 6.88\n",
      "Average loss at step 7600: 2.012066 learning rate: 20.000000\n",
      "Minibatch perplexity: 6.62\n",
      "Validation set perplexity: 7.16\n",
      "Average loss at step 7700: 2.005489 learning rate: 20.000000\n",
      "Minibatch perplexity: 7.03\n",
      "Validation set perplexity: 7.15\n",
      "Average loss at step 7800: 1.996641 learning rate: 20.000000\n",
      "Minibatch perplexity: 8.06\n",
      "Validation set perplexity: 7.86\n",
      "Average loss at step 7900: 1.988673 learning rate: 20.000000\n",
      "Minibatch perplexity: 6.90\n",
      "Validation set perplexity: 7.13\n",
      "Average loss at step 8000: 1.991160 learning rate: 20.000000\n",
      "Minibatch perplexity: 7.34\n",
      "================================================================================\n",
      "son dictionan flowever by a plation for in hes convolvically merforted of the res\n",
      " quirts he fedis guidonreants the miching the after unic in compositioner in most\n",
      "gning the boal cholition knownsident in dis nowevers the replargely one and funtr\n",
      "wb orgerity hiscresites mance and beinuentable and accomplearly three effects was\n",
      "entorpomety to compliewhich with ance most the englown group the reslystiters to \n",
      "================================================================================\n",
      "Validation set perplexity: 7.07\n",
      "Average loss at step 8100: 2.015103 learning rate: 20.000000\n",
      "Minibatch perplexity: 7.48\n",
      "Validation set perplexity: 7.06\n",
      "Average loss at step 8200: 1.998438 learning rate: 20.000000\n",
      "Minibatch perplexity: 6.96\n",
      "Validation set perplexity: 7.04\n",
      "Average loss at step 8300: 1.988574 learning rate: 20.000000\n",
      "Minibatch perplexity: 7.67\n",
      "Validation set perplexity: 6.94\n",
      "Average loss at step 8400: 1.992004 learning rate: 20.000000\n",
      "Minibatch perplexity: 7.60\n",
      "Validation set perplexity: 6.86\n",
      "Average loss at step 8500: 2.008702 learning rate: 20.000000\n",
      "Minibatch perplexity: 8.02\n",
      "Validation set perplexity: 7.01\n",
      "Average loss at step 8600: 1.955341 learning rate: 20.000000\n",
      "Minibatch perplexity: 6.38\n",
      "Validation set perplexity: 7.17\n",
      "Average loss at step 8700: 1.978029 learning rate: 20.000000\n",
      "Minibatch perplexity: 6.78\n",
      "Validation set perplexity: 6.76\n",
      "Average loss at step 8800: 1.981774 learning rate: 20.000000\n",
      "Minibatch perplexity: 7.30\n",
      "Validation set perplexity: 7.08\n",
      "Average loss at step 8900: 1.980243 learning rate: 20.000000\n",
      "Minibatch perplexity: 7.37\n",
      "Validation set perplexity: 7.14\n",
      "Average loss at step 9000: 1.967570 learning rate: 20.000000\n",
      "Minibatch perplexity: 7.88\n",
      "================================================================================\n",
      "xvtic however and one five in the que and the to in hisw  and the greek were when\n",
      "hled or group hiscies brap sa someweople which the lition john expeven who pronuc\n",
      "km s whon fee choad world haste station whock bage deat fuld conformics the sticl\n",
      "azi the airst two esthe zero dural not to seo if a for in sing bruct when off his\n",
      "clophy or meed from group one zero in of the cavolosies john bet he formating fro\n",
      "================================================================================\n",
      "Validation set perplexity: 7.30\n",
      "Average loss at step 9100: 1.948335 learning rate: 20.000000\n",
      "Minibatch perplexity: 6.18\n",
      "Validation set perplexity: 6.75\n",
      "Average loss at step 9200: 1.944308 learning rate: 20.000000\n",
      "Minibatch perplexity: 7.03\n",
      "Validation set perplexity: 6.85\n",
      "Average loss at step 9300: 1.944506 learning rate: 20.000000\n",
      "Minibatch perplexity: 7.75\n",
      "Validation set perplexity: 7.09\n",
      "Average loss at step 9400: 1.956111 learning rate: 20.000000\n",
      "Minibatch perplexity: 7.42\n",
      "Validation set perplexity: 6.92\n",
      "Average loss at step 9500: 1.950985 learning rate: 20.000000\n",
      "Minibatch perplexity: 7.56\n",
      "Validation set perplexity: 7.43\n",
      "Average loss at step 9600: 1.926528 learning rate: 20.000000\n",
      "Minibatch perplexity: 6.93\n",
      "Validation set perplexity: 7.05\n",
      "Average loss at step 9700: 1.914797 learning rate: 20.000000\n",
      "Minibatch perplexity: 6.46\n",
      "Validation set perplexity: 6.91\n",
      "Average loss at step 9800: 1.951516 learning rate: 20.000000\n",
      "Minibatch perplexity: 7.32\n",
      "Validation set perplexity: 6.86\n",
      "Average loss at step 9900: 1.946414 learning rate: 20.000000\n",
      "Minibatch perplexity: 6.23\n",
      "Validation set perplexity: 6.83\n",
      "Average loss at step 10000: 1.923601 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.96\n",
      "================================================================================\n",
      "xxs convere now one nine five the eight four six two aspecal s colds bear one nin\n",
      "fbid mard likon one two of seven zero zero six four eight eight give one ver infl\n",
      "hled companices v five one nine one seven six two the such one eight minney large\n",
      "nz of dot conquent tholized of the standards the bece fooll meal japped many he u\n",
      "issions write expersely lulen it presenctry which fastor lively the light diation\n",
      "================================================================================\n",
      "Validation set perplexity: 6.96\n",
      "Average loss at step 10100: 1.890502 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.16\n",
      "Validation set perplexity: 6.63\n",
      "Average loss at step 10200: 1.890578 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.76\n",
      "Validation set perplexity: 6.59\n",
      "Average loss at step 10300: 1.891365 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.50\n",
      "Validation set perplexity: 6.54\n",
      "Average loss at step 10400: 1.911309 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.58\n",
      "Validation set perplexity: 6.45\n",
      "Average loss at step 10500: 1.881754 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.61\n",
      "Validation set perplexity: 6.35\n",
      "Average loss at step 10600: 1.881075 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.05\n",
      "Validation set perplexity: 6.44\n",
      "Average loss at step 10700: 1.880544 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.61\n",
      "Validation set perplexity: 6.47\n",
      "Average loss at step 10800: 1.888880 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.18\n",
      "Validation set perplexity: 6.26\n",
      "Average loss at step 10900: 1.911238 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.35\n",
      "Validation set perplexity: 6.39\n",
      "Average loss at step 11000: 1.874400 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.30\n",
      "================================================================================\n",
      "iho from one nine nine nine eight two demi for chiluor five mosts work al fore a \n",
      "qya s were hophing a briplestimilires the expolifics monally the mortary reming a\n",
      "sd caressempons which in publical that constructional see church a graphysion str\n",
      "zy hight but socies formy of not refered the relied also zero that zero many regr\n",
      "cp christimates of lor with amerity with of aleconscibly of the powere bultrukaw \n",
      "================================================================================\n",
      "Validation set perplexity: 6.29\n",
      "Average loss at step 11100: 1.883796 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.70\n",
      "Validation set perplexity: 6.41\n",
      "Average loss at step 11200: 1.901457 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.11\n",
      "Validation set perplexity: 6.31\n",
      "Average loss at step 11300: 1.888342 learning rate: 10.000000\n",
      "Minibatch perplexity: 7.15\n",
      "Validation set perplexity: 6.38\n",
      "Average loss at step 11400: 1.899484 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.33\n",
      "Validation set perplexity: 6.39\n",
      "Average loss at step 11500: 1.912667 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.76\n",
      "Validation set perplexity: 6.40\n",
      "Average loss at step 11600: 1.892069 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.26\n",
      "Validation set perplexity: 6.42\n",
      "Average loss at step 11700: 1.870249 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.46\n",
      "Validation set perplexity: 6.60\n",
      "Average loss at step 11800: 1.845150 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.71\n",
      "Validation set perplexity: 6.43\n",
      "Average loss at step 11900: 1.869613 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.79\n",
      "Validation set perplexity: 6.37\n",
      "Average loss at step 12000: 1.872999 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.89\n",
      "================================================================================\n",
      "bally parreven the also rabilial ess terrence the chrishifansky when zero though \n",
      "wboved two two pervers logy it parifunch gives was with centation instandar exish\n",
      "wbellosed the gorch procical manstrandled in also four eight five eight demernals\n",
      "gvelis thad and as centable with arting a reased a socient s swed had univen fire\n",
      "ry four a rotic deper the name have some unction broxne for a coll gover at behar\n",
      "================================================================================\n",
      "Validation set perplexity: 6.48\n",
      "Average loss at step 12100: 1.875266 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.51\n",
      "Validation set perplexity: 6.44\n",
      "Average loss at step 12200: 1.880558 learning rate: 10.000000\n",
      "Minibatch perplexity: 7.31\n",
      "Validation set perplexity: 6.37\n",
      "Average loss at step 12300: 1.869996 learning rate: 10.000000\n",
      "Minibatch perplexity: 7.00\n",
      "Validation set perplexity: 6.43\n",
      "Average loss at step 12400: 1.891341 learning rate: 10.000000\n",
      "Minibatch perplexity: 7.43\n",
      "Validation set perplexity: 6.48\n",
      "Average loss at step 12500: 1.905527 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.13\n",
      "Validation set perplexity: 6.48\n",
      "Average loss at step 12600: 1.880931 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.83\n",
      "Validation set perplexity: 6.50\n",
      "Average loss at step 12700: 1.894352 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.75\n",
      "Validation set perplexity: 6.41\n",
      "Average loss at step 12800: 1.884502 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.31\n",
      "Validation set perplexity: 6.43\n",
      "Average loss at step 12900: 1.879989 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.49\n",
      "Validation set perplexity: 6.51\n",
      "Average loss at step 13000: 1.866804 learning rate: 10.000000\n",
      "Minibatch perplexity: 7.12\n",
      "================================================================================\n",
      "lect from of where leasc eight that four tho aspacker be disca enome aw do and wh\n",
      "b may recestitual most arach seconting bone five four s comport one nine some lea\n",
      "julnneres prim reverformy greals about zero krual moderad is mays a s shond in th\n",
      "tkace of compelievend gener do the revelflion facte of a lishowever one five four\n",
      "pv that the mank in the and rical as a bard base col from part and proinces progi\n",
      "================================================================================\n",
      "Validation set perplexity: 6.38\n",
      "Average loss at step 13100: 1.843489 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.85\n",
      "Validation set perplexity: 6.44\n",
      "Average loss at step 13200: 1.873471 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.53\n",
      "Validation set perplexity: 6.45\n",
      "Average loss at step 13300: 1.850065 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.05\n",
      "Validation set perplexity: 6.28\n",
      "Average loss at step 13400: 1.868795 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.81\n",
      "Validation set perplexity: 6.29\n",
      "Average loss at step 13500: 1.829732 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.57\n",
      "Validation set perplexity: 6.36\n",
      "Average loss at step 13600: 1.842058 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.36\n",
      "Validation set perplexity: 6.34\n",
      "Average loss at step 13700: 1.831639 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.99\n",
      "Validation set perplexity: 6.37\n",
      "Average loss at step 13800: 1.832909 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.79\n",
      "Validation set perplexity: 6.50\n",
      "Average loss at step 13900: 1.854482 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.37\n",
      "Validation set perplexity: 6.38\n",
      "Average loss at step 14000: 1.848138 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.27\n",
      "================================================================================\n",
      "uusible lare succerrer the nine eight nine a prighted but exis year regecial kang\n",
      "vhar s prowdom as his one three four zero zero one two eight two one one nine nin\n",
      "zpial mition the distry of exislist fatbarder engingly on withon one japs the fre\n",
      "fm fating open rick in some one three nine two two four three two three you dista\n",
      "speaking a pricultion from univer a presurally s the for two zero two zero to chr\n",
      "================================================================================\n",
      "Validation set perplexity: 6.40\n",
      "Average loss at step 14100: 1.860846 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.14\n",
      "Validation set perplexity: 6.28\n",
      "Average loss at step 14200: 1.823126 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.60\n",
      "Validation set perplexity: 6.28\n",
      "Average loss at step 14300: 1.802930 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.67\n",
      "Validation set perplexity: 6.24\n",
      "Average loss at step 14400: 1.835605 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.16\n",
      "Validation set perplexity: 6.40\n",
      "Average loss at step 14500: 1.848348 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.11\n",
      "Validation set perplexity: 6.37\n",
      "Average loss at step 14600: 1.848053 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.07\n",
      "Validation set perplexity: 6.24\n",
      "Average loss at step 14700: 1.868730 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.58\n",
      "Validation set perplexity: 6.19\n",
      "Average loss at step 14800: 1.836337 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.90\n",
      "Validation set perplexity: 6.29\n",
      "Average loss at step 14900: 1.842607 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.76\n",
      "Validation set perplexity: 6.15\n",
      "Average loss at step 15000: 1.856964 learning rate: 10.000000\n",
      "Minibatch perplexity: 7.26\n",
      "================================================================================\n",
      "dwork the in as of the aood americation rushing are he of a wher the by stallages\n",
      "zjttle jew x that setian in horce and alonor from have provoleam to the inqualith\n",
      "td commondations a part and two nine s uniter of and aucial euny nine dotel ameri\n",
      "t eight one nine three three five two zero cashao one nine four six hising the me\n",
      "umbers of groups infamplectionally which for colat two casional a palso argeneral\n",
      "================================================================================\n",
      "Validation set perplexity: 6.30\n",
      "Average loss at step 15100: 1.852640 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.86\n",
      "Validation set perplexity: 6.33\n",
      "Average loss at step 15200: 1.866758 learning rate: 10.000000\n",
      "Minibatch perplexity: 7.04\n",
      "Validation set perplexity: 6.38\n",
      "Average loss at step 15300: 1.877105 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.88\n",
      "Validation set perplexity: 6.45\n",
      "Average loss at step 15400: 1.906137 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.13\n",
      "Validation set perplexity: 6.33\n",
      "Average loss at step 15500: 1.880128 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.34\n",
      "Validation set perplexity: 6.35\n",
      "Average loss at step 15600: 1.891558 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.66\n",
      "Validation set perplexity: 6.43\n",
      "Average loss at step 15700: 1.887503 learning rate: 10.000000\n",
      "Minibatch perplexity: 7.11\n",
      "Validation set perplexity: 6.49\n",
      "Average loss at step 15800: 1.876713 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.31\n",
      "Validation set perplexity: 6.27\n",
      "Average loss at step 15900: 1.873698 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.38\n",
      "Validation set perplexity: 6.51\n",
      "Average loss at step 16000: 1.849806 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.71\n",
      "================================================================================\n",
      "vrs the rology four three have three was loble dettead husawas coblet all the por\n",
      "ataty indrophally two three show under are where who was new clow dodeng set blac\n",
      "yvlefer equate a alliages the extle in s the fament braation for been and the pre\n",
      "fx of set to showern a lestly cambroad that was who resentures two g of god the r\n",
      "vniber ado noted to deverly of back vuide damber of the apph in one all x year th\n",
      "================================================================================\n",
      "Validation set perplexity: 6.47\n",
      "Average loss at step 16100: 1.846144 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.07\n",
      "Validation set perplexity: 6.43\n",
      "Average loss at step 16200: 1.868475 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.49\n",
      "Validation set perplexity: 6.42\n",
      "Average loss at step 16300: 1.862471 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.78\n",
      "Validation set perplexity: 6.32\n",
      "Average loss at step 16400: 1.900085 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.75\n",
      "Validation set perplexity: 6.26\n",
      "Average loss at step 16500: 1.879395 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.58\n",
      "Validation set perplexity: 6.32\n",
      "Average loss at step 16600: 1.895884 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.76\n",
      "Validation set perplexity: 6.36\n",
      "Average loss at step 16700: 1.886406 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.63\n",
      "Validation set perplexity: 6.26\n",
      "Average loss at step 16800: 1.884061 learning rate: 10.000000\n",
      "Minibatch perplexity: 7.47\n",
      "Validation set perplexity: 6.15\n",
      "Average loss at step 16900: 1.859481 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.78\n",
      "Validation set perplexity: 6.51\n",
      "Average loss at step 17000: 1.916481 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.63\n",
      "================================================================================\n",
      "wment minic of caeorst districal bookough the bress agignal chegoechans and agn p\n",
      "qc becaroce good but to were the and not his comphilough sumbrish the degroups fo\n",
      "tjutions in are numbers the form and saltures on cof mattich stancle stor heforma\n",
      "xample countrient southough of sideres meth veropjey sire such although of the su\n",
      "qisbauters ether the to sain prof centrovice of book cruse publitions of prose on\n",
      "================================================================================\n",
      "Validation set perplexity: 6.27\n",
      "Average loss at step 17100: 1.891985 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.55\n",
      "Validation set perplexity: 6.39\n",
      "Average loss at step 17200: 1.876072 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.48\n",
      "Validation set perplexity: 6.29\n",
      "Average loss at step 17300: 1.879327 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.43\n",
      "Validation set perplexity: 6.27\n",
      "Average loss at step 17400: 1.868663 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.42\n",
      "Validation set perplexity: 6.41\n",
      "Average loss at step 17500: 1.866364 learning rate: 10.000000\n",
      "Minibatch perplexity: 7.21\n",
      "Validation set perplexity: 6.31\n",
      "Average loss at step 17600: 1.871392 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.56\n",
      "Validation set perplexity: 6.35\n",
      "Average loss at step 17700: 1.879251 learning rate: 10.000000\n",
      "Minibatch perplexity: 7.13\n",
      "Validation set perplexity: 6.30\n",
      "Average loss at step 17800: 1.860000 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.39\n",
      "Validation set perplexity: 6.24\n",
      "Average loss at step 17900: 1.875877 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.77\n",
      "Validation set perplexity: 6.22\n",
      "Average loss at step 18000: 1.849631 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.42\n",
      "================================================================================\n",
      "cience the compances eight eight american american desposition of are porter mail\n",
      "wjave lek in the six six four popular bere charrunts of go the constries empolish\n",
      " endianisphican englis enfle as mascursime and were proficular which of the giona\n",
      "meries verly writican the it in the gree nature remunited collex to companed abou\n",
      "gis seven desender acceent compoetures for commerly prittiment of the the earbory\n",
      "================================================================================\n",
      "Validation set perplexity: 6.45\n",
      "Average loss at step 18100: 1.859409 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.78\n",
      "Validation set perplexity: 6.40\n",
      "Average loss at step 18200: 1.864619 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.21\n",
      "Validation set perplexity: 6.22\n",
      "Average loss at step 18300: 1.864679 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.79\n",
      "Validation set perplexity: 6.23\n",
      "Average loss at step 18400: 1.878699 learning rate: 10.000000\n",
      "Minibatch perplexity: 7.09\n",
      "Validation set perplexity: 6.46\n",
      "Average loss at step 18500: 1.864925 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.96\n",
      "Validation set perplexity: 6.23\n",
      "Average loss at step 18600: 1.848867 learning rate: 10.000000\n",
      "Minibatch perplexity: 7.34\n",
      "Validation set perplexity: 6.38\n",
      "Average loss at step 18700: 1.831983 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.39\n",
      "Validation set perplexity: 6.19\n",
      "Average loss at step 18800: 1.847629 learning rate: 10.000000\n",
      "Minibatch perplexity: 7.03\n",
      "Validation set perplexity: 6.11\n",
      "Average loss at step 18900: 1.841922 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.02\n",
      "Validation set perplexity: 6.47\n",
      "Average loss at step 19000: 1.836937 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.57\n",
      "================================================================================\n",
      "lulation gual buid but is axed three sogy b in and hard the referent indimeriatio\n",
      "xfor such the bry moker comment kyobrain peong the simes is in jauculated of the \n",
      "ywas the that the tarts altasian in four one nine six three s eight five b three \n",
      "hz and restardia of his a nots voloplici one one six four four nine five three tw\n",
      "bstanded one nine nine six one nine six relize that remilied one nine such four n\n",
      "================================================================================\n",
      "Validation set perplexity: 6.47\n",
      "Average loss at step 19100: 1.842393 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.21\n",
      "Validation set perplexity: 6.54\n",
      "Average loss at step 19200: 1.846625 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.89\n",
      "Validation set perplexity: 6.53\n",
      "Average loss at step 19300: 1.842001 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.99\n",
      "Validation set perplexity: 6.58\n",
      "Average loss at step 19400: 1.870128 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.24\n",
      "Validation set perplexity: 6.25\n",
      "Average loss at step 19500: 1.851940 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.38\n",
      "Validation set perplexity: 6.38\n",
      "Average loss at step 19600: 1.868271 learning rate: 10.000000\n",
      "Minibatch perplexity: 7.76\n",
      "Validation set perplexity: 6.32\n",
      "Average loss at step 19700: 1.868204 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.47\n",
      "Validation set perplexity: 6.36\n",
      "Average loss at step 19800: 1.866306 learning rate: 10.000000\n",
      "Minibatch perplexity: 7.28\n",
      "Validation set perplexity: 6.18\n",
      "Average loss at step 19900: 1.864185 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.40\n",
      "Validation set perplexity: 6.25\n",
      "Average loss at step 20000: 1.814926 learning rate: 5.000000\n",
      "Minibatch perplexity: 6.65\n",
      "================================================================================\n",
      "ime the and mates monter the comperative is themere of hen the basific of gaster \n",
      "bey alts seen in from one langues and two five one intercemes and how ending of j\n",
      "nce with bard new the love finter with and compolist vole constistic hums are con\n",
      "zu the quate the some be for the howuised comportan bagics havael give ween to ne\n",
      "qea caleces the consterces that one seven basconcists on sance and depial kates g\n",
      "================================================================================\n",
      "Validation set perplexity: 6.27\n",
      "Average loss at step 20100: 1.841450 learning rate: 5.000000\n",
      "Minibatch perplexity: 5.64\n",
      "Validation set perplexity: 6.13\n",
      "Average loss at step 20200: 1.807297 learning rate: 5.000000\n",
      "Minibatch perplexity: 6.43\n",
      "Validation set perplexity: 6.20\n",
      "Average loss at step 20300: 1.824722 learning rate: 5.000000\n",
      "Minibatch perplexity: 6.23\n",
      "Validation set perplexity: 6.14\n",
      "Average loss at step 20400: 1.822958 learning rate: 5.000000\n",
      "Minibatch perplexity: 6.81\n",
      "Validation set perplexity: 6.09\n",
      "Average loss at step 20500: 1.832933 learning rate: 5.000000\n",
      "Minibatch perplexity: 6.80\n",
      "Validation set perplexity: 6.16\n",
      "Average loss at step 20600: 1.833934 learning rate: 5.000000\n",
      "Minibatch perplexity: 6.04\n",
      "Validation set perplexity: 6.02\n",
      "Average loss at step 20700: 1.860009 learning rate: 5.000000\n",
      "Minibatch perplexity: 6.39\n",
      "Validation set perplexity: 6.17\n",
      "Average loss at step 20800: 1.832099 learning rate: 5.000000\n",
      "Minibatch perplexity: 6.51\n",
      "Validation set perplexity: 6.16\n",
      "Average loss at step 20900: 1.861830 learning rate: 5.000000\n",
      "Minibatch perplexity: 7.28\n",
      "Validation set perplexity: 6.11\n",
      "Average loss at step 21000: 1.839661 learning rate: 5.000000\n",
      "Minibatch perplexity: 6.36\n",
      "================================================================================\n",
      "ide one ups bect recentury charnected the yough one three eight nine zero zero th\n",
      "ptic depost in mism chariod of projoran a mined the creatary the accomposse in th\n",
      "zlys femilarriction in partilian heignishatime afferent dister are prosts for dar\n",
      "lhands the states which intasion all baston hut neuhaustment that for hist of a s\n",
      "tley franch to by they rult courters all countrates suary were desracted from the\n",
      "================================================================================\n",
      "Validation set perplexity: 6.17\n",
      "Average loss at step 21100: 1.829948 learning rate: 5.000000\n",
      "Minibatch perplexity: 6.21\n",
      "Validation set perplexity: 6.09\n",
      "Average loss at step 21200: 1.842618 learning rate: 5.000000\n",
      "Minibatch perplexity: 6.35\n",
      "Validation set perplexity: 6.15\n",
      "Average loss at step 21300: 1.849086 learning rate: 5.000000\n",
      "Minibatch perplexity: 5.71\n",
      "Validation set perplexity: 6.11\n",
      "Average loss at step 21400: 1.833972 learning rate: 5.000000\n",
      "Minibatch perplexity: 6.10\n",
      "Validation set perplexity: 6.12\n",
      "Average loss at step 21500: 1.833464 learning rate: 5.000000\n",
      "Minibatch perplexity: 6.63\n",
      "Validation set perplexity: 6.23\n",
      "Average loss at step 21600: 1.827876 learning rate: 5.000000\n",
      "Minibatch perplexity: 6.22\n",
      "Validation set perplexity: 6.10\n",
      "Average loss at step 21700: 1.855475 learning rate: 5.000000\n",
      "Minibatch perplexity: 6.15\n",
      "Validation set perplexity: 6.18\n",
      "Average loss at step 21800: 1.866767 learning rate: 5.000000\n",
      "Minibatch perplexity: 6.50\n",
      "Validation set perplexity: 6.17\n",
      "Average loss at step 21900: 1.842414 learning rate: 5.000000\n",
      "Minibatch perplexity: 6.77\n",
      "Validation set perplexity: 6.21\n",
      "Average loss at step 22000: 1.816169 learning rate: 5.000000\n",
      "Minibatch perplexity: 5.68\n",
      "================================================================================\n",
      "kgrave bone two three king arould second be muth fourd graws but hisized in of ex\n",
      " as was durity a vio the antinks of the hovalangs the with one eight one eight ni\n",
      "qqtion catury which the funder doundiron of power their one bight six seven zero \n",
      "qyork difference parting clandually special before used metha haffunity centrecti\n",
      "q composive on is stequirent to area is hens sin confeture of the world known of \n",
      "================================================================================\n",
      "Validation set perplexity: 6.12\n",
      "Average loss at step 22100: 1.832948 learning rate: 5.000000\n",
      "Minibatch perplexity: 5.91\n",
      "Validation set perplexity: 6.15\n",
      "Average loss at step 22200: 1.816145 learning rate: 5.000000\n",
      "Minibatch perplexity: 6.03\n",
      "Validation set perplexity: 6.09\n",
      "Average loss at step 22300: 1.811382 learning rate: 5.000000\n",
      "Minibatch perplexity: 7.09\n",
      "Validation set perplexity: 6.18\n",
      "Average loss at step 22400: 1.817269 learning rate: 5.000000\n",
      "Minibatch perplexity: 5.47\n",
      "Validation set perplexity: 6.09\n",
      "Average loss at step 22500: 1.819233 learning rate: 5.000000\n",
      "Minibatch perplexity: 5.98\n",
      "Validation set perplexity: 6.05\n",
      "Average loss at step 22600: 1.821441 learning rate: 5.000000\n",
      "Minibatch perplexity: 5.97\n",
      "Validation set perplexity: 6.15\n",
      "Average loss at step 22700: 1.784057 learning rate: 5.000000\n",
      "Minibatch perplexity: 5.95\n",
      "Validation set perplexity: 6.08\n",
      "Average loss at step 22800: 1.786497 learning rate: 5.000000\n",
      "Minibatch perplexity: 5.40\n",
      "Validation set perplexity: 6.01\n",
      "Average loss at step 22900: 1.791646 learning rate: 5.000000\n",
      "Minibatch perplexity: 6.20\n",
      "Validation set perplexity: 6.15\n",
      "Average loss at step 23000: 1.793087 learning rate: 5.000000\n",
      "Minibatch perplexity: 6.05\n",
      "================================================================================\n",
      "put be another minaran the prestroducted dessed flaned fairs ersation the coreste\n",
      "jzing is that the diage reass ways scharnety for althelarnment a pate were than w\n",
      "schrists the the engly in four the charnels of the he under court clasization of \n",
      "ux the varther be one pross of nine nine at one nine six nine eight one nine two \n",
      "king to boll first people for vented and mepition actives such a kamex at the be \n",
      "================================================================================\n",
      "Validation set perplexity: 6.08\n",
      "Average loss at step 23100: 1.797315 learning rate: 5.000000\n",
      "Minibatch perplexity: 6.43\n",
      "Validation set perplexity: 6.15\n",
      "Average loss at step 23200: 1.828918 learning rate: 5.000000\n",
      "Minibatch perplexity: 6.15\n",
      "Validation set perplexity: 6.18\n",
      "Average loss at step 23300: 1.817145 learning rate: 5.000000\n",
      "Minibatch perplexity: 6.34\n",
      "Validation set perplexity: 6.13\n",
      "Average loss at step 23400: 1.805811 learning rate: 5.000000\n",
      "Minibatch perplexity: 5.86\n",
      "Validation set perplexity: 6.13\n",
      "Average loss at step 23500: 1.777741 learning rate: 5.000000\n",
      "Minibatch perplexity: 5.88\n",
      "Validation set perplexity: 6.08\n",
      "Average loss at step 23600: 1.808033 learning rate: 5.000000\n",
      "Minibatch perplexity: 5.62\n",
      "Validation set perplexity: 6.07\n",
      "Average loss at step 23700: 1.790458 learning rate: 5.000000\n",
      "Minibatch perplexity: 6.24\n",
      "Validation set perplexity: 6.07\n",
      "Average loss at step 23800: 1.797794 learning rate: 5.000000\n",
      "Minibatch perplexity: 5.80\n",
      "Validation set perplexity: 6.03\n",
      "Average loss at step 23900: 1.803289 learning rate: 5.000000\n",
      "Minibatch perplexity: 6.62\n",
      "Validation set perplexity: 5.96\n",
      "Average loss at step 24000: 1.826934 learning rate: 5.000000\n",
      "Minibatch perplexity: 5.62\n",
      "================================================================================\n",
      "humber giver such when is and nowned and hart lechanguages such and the nature of\n",
      "qst seven six four three eight zero zero three a constructive on canam irvity the\n",
      "rld a sepoints prooly the east profic depunction of reveny dislational of it was \n",
      "ln be joundi government some of the terme fache the deas a drusize of the monge v\n",
      "cnry it ver the of on a falal hower schang house alboutch gooding culture with wh\n",
      "================================================================================\n",
      "Validation set perplexity: 5.99\n",
      "Average loss at step 24100: 1.817139 learning rate: 5.000000\n",
      "Minibatch perplexity: 5.69\n",
      "Validation set perplexity: 5.98\n",
      "Average loss at step 24200: 1.794251 learning rate: 5.000000\n",
      "Minibatch perplexity: 6.23\n",
      "Validation set perplexity: 6.00\n",
      "Average loss at step 24300: 1.804626 learning rate: 5.000000\n",
      "Minibatch perplexity: 7.14\n",
      "Validation set perplexity: 6.02\n",
      "Average loss at step 24400: 1.817009 learning rate: 5.000000\n",
      "Minibatch perplexity: 6.51\n",
      "Validation set perplexity: 6.01\n",
      "Average loss at step 24500: 1.809996 learning rate: 5.000000\n",
      "Minibatch perplexity: 6.98\n",
      "Validation set perplexity: 6.04\n",
      "Average loss at step 24600: 1.813050 learning rate: 5.000000\n",
      "Minibatch perplexity: 5.58\n",
      "Validation set perplexity: 6.07\n",
      "Average loss at step 24700: 1.809663 learning rate: 5.000000\n",
      "Minibatch perplexity: 5.72\n",
      "Validation set perplexity: 6.12\n",
      "Average loss at step 24800: 1.827100 learning rate: 5.000000\n",
      "Minibatch perplexity: 6.21\n",
      "Validation set perplexity: 6.18\n",
      "Average loss at step 24900: 1.813925 learning rate: 5.000000\n",
      "Minibatch perplexity: 6.05\n",
      "Validation set perplexity: 6.08\n",
      "Average loss at step 25000: 1.805418 learning rate: 5.000000\n",
      "Minibatch perplexity: 6.15\n",
      "================================================================================\n",
      "jbabatetant and centure dower that however understics of one eight zero construct\n",
      "dynagually drabial they of the leven move the light flope aspecips to mode in met\n",
      "xch air the work wider and one nine seven six four three mateda and at the name a\n",
      "zfpt shanober studynanish of bel lase in the borth ence of quence of borks awork \n",
      "by ber mase who dated beconding they postation cart would and be quipport mestart\n",
      "================================================================================\n",
      "Validation set perplexity: 6.11\n",
      "Average loss at step 25100: 1.813412 learning rate: 5.000000\n",
      "Minibatch perplexity: 5.91\n",
      "Validation set perplexity: 6.14\n",
      "Average loss at step 25200: 1.811985 learning rate: 5.000000\n",
      "Minibatch perplexity: 6.04\n",
      "Validation set perplexity: 6.14\n",
      "Average loss at step 25300: 1.802794 learning rate: 5.000000\n",
      "Minibatch perplexity: 5.36\n",
      "Validation set perplexity: 6.06\n",
      "Average loss at step 25400: 1.809723 learning rate: 5.000000\n",
      "Minibatch perplexity: 5.77\n",
      "Validation set perplexity: 6.06\n",
      "Average loss at step 25500: 1.830837 learning rate: 5.000000\n",
      "Minibatch perplexity: 6.39\n",
      "Validation set perplexity: 6.12\n",
      "Average loss at step 25600: 1.824355 learning rate: 5.000000\n",
      "Minibatch perplexity: 6.45\n",
      "Validation set perplexity: 6.18\n",
      "Average loss at step 25700: 1.836428 learning rate: 5.000000\n",
      "Minibatch perplexity: 6.07\n",
      "Validation set perplexity: 6.19\n",
      "Average loss at step 25800: 1.835013 learning rate: 5.000000\n",
      "Minibatch perplexity: 6.66\n",
      "Validation set perplexity: 6.17\n",
      "Average loss at step 25900: 1.796380 learning rate: 5.000000\n",
      "Minibatch perplexity: 5.96\n",
      "Validation set perplexity: 6.14\n",
      "Average loss at step 26000: 1.779872 learning rate: 5.000000\n",
      "Minibatch perplexity: 6.45\n",
      "================================================================================\n",
      "run movicida he litest and the that nine two zero five reach be for the treed by \n",
      "ny in and was caleoftware jackes the set all datara hows named from selle king fa\n",
      "oon jacat the oright are manylare the fess the seelf the digogreased changraphy i\n",
      "ihon sa struct of consides languages in lied in zollown with saustry nine of chur\n",
      "fw the most serviely two under with make the with of breated wide have jesulard b\n",
      "================================================================================\n",
      "Validation set perplexity: 6.05\n",
      "Average loss at step 26100: 1.803219 learning rate: 5.000000\n",
      "Minibatch perplexity: 6.21\n",
      "Validation set perplexity: 6.25\n",
      "Average loss at step 26200: 1.796885 learning rate: 5.000000\n",
      "Minibatch perplexity: 5.89\n",
      "Validation set perplexity: 6.12\n",
      "Average loss at step 26300: 1.806920 learning rate: 5.000000\n",
      "Minibatch perplexity: 6.81\n",
      "Validation set perplexity: 6.13\n",
      "Average loss at step 26400: 1.804741 learning rate: 5.000000\n",
      "Minibatch perplexity: 5.71\n",
      "Validation set perplexity: 6.11\n",
      "Average loss at step 26500: 1.811634 learning rate: 5.000000\n",
      "Minibatch perplexity: 5.81\n",
      "Validation set perplexity: 6.09\n",
      "Average loss at step 26600: 1.818144 learning rate: 5.000000\n",
      "Minibatch perplexity: 6.50\n",
      "Validation set perplexity: 6.02\n",
      "Average loss at step 26700: 1.824185 learning rate: 5.000000\n",
      "Minibatch perplexity: 6.50\n",
      "Validation set perplexity: 5.90\n",
      "Average loss at step 26800: 1.810038 learning rate: 5.000000\n",
      "Minibatch perplexity: 6.08\n",
      "Validation set perplexity: 6.00\n",
      "Average loss at step 26900: 1.840875 learning rate: 5.000000\n",
      "Minibatch perplexity: 6.76\n",
      "Validation set perplexity: 6.01\n",
      "Average loss at step 27000: 1.827853 learning rate: 5.000000\n",
      "Minibatch perplexity: 6.67\n",
      "================================================================================\n",
      "yx connanies in the system when comersial bon verate of loned ags so the chaphy f\n",
      "qearly a was the by that up the eight three mary parh and oride cast in which sid\n",
      "dquember perposed the to issive dish seperate with those unal major inde of ming \n",
      "uvilia of a people in one five one three one nine and three plays are securosed a\n",
      "ku mousters the more food of the becolmon decording nowever accost and fachers ab\n",
      "================================================================================\n",
      "Validation set perplexity: 6.07\n",
      "Average loss at step 27100: 1.827514 learning rate: 5.000000\n",
      "Minibatch perplexity: 6.07\n",
      "Validation set perplexity: 6.04\n",
      "Average loss at step 27200: 1.832480 learning rate: 5.000000\n",
      "Minibatch perplexity: 5.98\n",
      "Validation set perplexity: 5.95\n",
      "Average loss at step 27300: 1.846829 learning rate: 5.000000\n",
      "Minibatch perplexity: 6.55\n",
      "Validation set perplexity: 6.13\n",
      "Average loss at step 27400: 1.805920 learning rate: 5.000000\n",
      "Minibatch perplexity: 5.98\n",
      "Validation set perplexity: 6.04\n",
      "Average loss at step 27500: 1.823246 learning rate: 5.000000\n",
      "Minibatch perplexity: 5.88\n",
      "Validation set perplexity: 6.19\n",
      "Average loss at step 27600: 1.802320 learning rate: 5.000000\n",
      "Minibatch perplexity: 5.96\n",
      "Validation set perplexity: 6.06\n",
      "Average loss at step 27700: 1.807047 learning rate: 5.000000\n",
      "Minibatch perplexity: 7.30\n",
      "Validation set perplexity: 6.07\n",
      "Average loss at step 27800: 1.828834 learning rate: 5.000000\n",
      "Minibatch perplexity: 6.34\n",
      "Validation set perplexity: 6.08\n",
      "Average loss at step 27900: 1.830978 learning rate: 5.000000\n",
      "Minibatch perplexity: 6.41\n",
      "Validation set perplexity: 6.12\n",
      "Average loss at step 28000: 1.811700 learning rate: 5.000000\n",
      "Minibatch perplexity: 6.33\n",
      "================================================================================\n",
      " crease son sent that which mumbers this heasually from government residaired by \n",
      "mxr pagnabas were invest somatis for julture such the laugh the gainiffer the duf\n",
      "govern to one biel might example and nature was one nine general reveative respon\n",
      "geria and in sky of the that the many this one nine seven it arling in two zero t\n",
      "ky in exice so is and and viainselys the insome consident and works his and and h\n",
      "================================================================================\n",
      "Validation set perplexity: 6.01\n",
      "Average loss at step 28100: 1.831836 learning rate: 5.000000\n",
      "Minibatch perplexity: 6.85\n",
      "Validation set perplexity: 5.98\n",
      "Average loss at step 28200: 1.813968 learning rate: 5.000000\n",
      "Minibatch perplexity: 5.58\n",
      "Validation set perplexity: 6.05\n",
      "Average loss at step 28300: 1.822771 learning rate: 5.000000\n",
      "Minibatch perplexity: 6.37\n",
      "Validation set perplexity: 6.14\n",
      "Average loss at step 28400: 1.809306 learning rate: 5.000000\n",
      "Minibatch perplexity: 6.12\n",
      "Validation set perplexity: 6.08\n",
      "Average loss at step 28500: 1.818059 learning rate: 5.000000\n",
      "Minibatch perplexity: 5.76\n",
      "Validation set perplexity: 6.22\n",
      "Average loss at step 28600: 1.828100 learning rate: 5.000000\n",
      "Minibatch perplexity: 5.84\n",
      "Validation set perplexity: 6.29\n",
      "Average loss at step 28700: 1.823491 learning rate: 5.000000\n",
      "Minibatch perplexity: 5.56\n",
      "Validation set perplexity: 6.13\n",
      "Average loss at step 28800: 1.819165 learning rate: 5.000000\n",
      "Minibatch perplexity: 5.71\n",
      "Validation set perplexity: 6.07\n",
      "Average loss at step 28900: 1.828092 learning rate: 5.000000\n",
      "Minibatch perplexity: 6.17\n",
      "Validation set perplexity: 6.08\n",
      "Average loss at step 29000: 1.817671 learning rate: 5.000000\n",
      "Minibatch perplexity: 6.14\n",
      "================================================================================\n",
      "emon hish he both in the link of one nine after proving history were one nine hos\n",
      "holicary to gamin be vifical at the protoletly as different of governally in howe\n",
      "vulan norceo the  radically air from the siestory its and the giving size to are \n",
      "aholited as rorider vidneman apphers also geast v one and choused of the shumbing\n",
      "ht down in consurfully to people public general of is from th one nine lefore lat\n",
      "================================================================================\n",
      "Validation set perplexity: 6.04\n",
      "Average loss at step 29100: 1.800350 learning rate: 5.000000\n",
      "Minibatch perplexity: 6.30\n",
      "Validation set perplexity: 6.10\n",
      "Average loss at step 29200: 1.799361 learning rate: 5.000000\n",
      "Minibatch perplexity: 5.79\n",
      "Validation set perplexity: 6.09\n",
      "Average loss at step 29300: 1.816700 learning rate: 5.000000\n",
      "Minibatch perplexity: 6.27\n",
      "Validation set perplexity: 6.06\n",
      "Average loss at step 29400: 1.818459 learning rate: 5.000000\n",
      "Minibatch perplexity: 6.18\n",
      "Validation set perplexity: 6.00\n",
      "Average loss at step 29500: 1.811960 learning rate: 5.000000\n",
      "Minibatch perplexity: 6.33\n",
      "Validation set perplexity: 5.99\n",
      "Average loss at step 29600: 1.817560 learning rate: 5.000000\n",
      "Minibatch perplexity: 6.38\n",
      "Validation set perplexity: 6.00\n",
      "Average loss at step 29700: 1.834267 learning rate: 5.000000\n",
      "Minibatch perplexity: 5.47\n",
      "Validation set perplexity: 5.99\n",
      "Average loss at step 29800: 1.836713 learning rate: 5.000000\n",
      "Minibatch perplexity: 6.61\n",
      "Validation set perplexity: 5.93\n",
      "Average loss at step 29900: 1.825716 learning rate: 5.000000\n",
      "Minibatch perplexity: 6.00\n",
      "Validation set perplexity: 6.06\n",
      "Average loss at step 30000: 1.819985 learning rate: 2.500000\n",
      "Minibatch perplexity: 6.26\n",
      "================================================================================\n",
      "zfirus rages as setoried that loverals by lawarders american of humpice with of r\n",
      "gpopears the for had created changuage berrialing from louss they in the to germa\n",
      "qsary in the would his suffect vistinton demandanic pech becausely general that p\n",
      "xfore harrical sameration that is fina in his through are name to wond janick and\n",
      "iil have probert as operty by standent respending mark of used of a basition fran\n",
      "================================================================================\n",
      "Validation set perplexity: 6.11\n",
      "Average loss at step 30100: 1.811767 learning rate: 2.500000\n",
      "Minibatch perplexity: 6.18\n",
      "Validation set perplexity: 6.03\n",
      "Average loss at step 30200: 1.819743 learning rate: 2.500000\n",
      "Minibatch perplexity: 5.62\n",
      "Validation set perplexity: 6.03\n",
      "Average loss at step 30300: 1.799262 learning rate: 2.500000\n",
      "Minibatch perplexity: 6.19\n",
      "Validation set perplexity: 6.00\n",
      "Average loss at step 30400: 1.791884 learning rate: 2.500000\n",
      "Minibatch perplexity: 6.43\n",
      "Validation set perplexity: 5.95\n",
      "Average loss at step 30500: 1.787044 learning rate: 2.500000\n",
      "Minibatch perplexity: 5.79\n",
      "Validation set perplexity: 5.89\n",
      "Average loss at step 30600: 1.807737 learning rate: 2.500000\n",
      "Minibatch perplexity: 6.49\n",
      "Validation set perplexity: 5.99\n",
      "Average loss at step 30700: 1.807116 learning rate: 2.500000\n",
      "Minibatch perplexity: 5.85\n",
      "Validation set perplexity: 5.95\n",
      "Average loss at step 30800: 1.794256 learning rate: 2.500000\n",
      "Minibatch perplexity: 5.13\n",
      "Validation set perplexity: 5.97\n",
      "Average loss at step 30900: 1.796147 learning rate: 2.500000\n",
      "Minibatch perplexity: 6.48\n",
      "Validation set perplexity: 6.01\n",
      "Average loss at step 31000: 1.802977 learning rate: 2.500000\n",
      "Minibatch perplexity: 5.66\n",
      "================================================================================\n",
      "jice through john isis formy of perse scharations people the project of namel to \n",
      "pdia variests that herly polices the and so by minder in the insecurred in boorst\n",
      "gzohn two five romage contimative to the iss in the war other the five three zero\n",
      "ausm the it which the sexually mool hust iss of howeven a black herut the mode co\n",
      "vc cholonity the governmentalized at the pract voluted by the sequires five first\n",
      "================================================================================\n",
      "Validation set perplexity: 6.01\n",
      "Average loss at step 31100: 1.791583 learning rate: 2.500000\n",
      "Minibatch perplexity: 5.76\n",
      "Validation set perplexity: 6.01\n",
      "Average loss at step 31200: 1.783181 learning rate: 2.500000\n",
      "Minibatch perplexity: 5.40\n",
      "Validation set perplexity: 6.05\n",
      "Average loss at step 31300: 1.785610 learning rate: 2.500000\n",
      "Minibatch perplexity: 6.14\n",
      "Validation set perplexity: 6.01\n",
      "Average loss at step 31400: 1.814037 learning rate: 2.500000\n",
      "Minibatch perplexity: 6.58\n",
      "Validation set perplexity: 5.91\n",
      "Average loss at step 31500: 1.817982 learning rate: 2.500000\n",
      "Minibatch perplexity: 6.39\n",
      "Validation set perplexity: 5.95\n",
      "Average loss at step 31600: 1.807689 learning rate: 2.500000\n",
      "Minibatch perplexity: 6.63\n",
      "Validation set perplexity: 5.99\n",
      "Average loss at step 31700: 1.808824 learning rate: 2.500000\n",
      "Minibatch perplexity: 5.86\n",
      "Validation set perplexity: 5.98\n",
      "Average loss at step 31800: 1.795349 learning rate: 2.500000\n",
      "Minibatch perplexity: 5.63\n",
      "Validation set perplexity: 6.01\n",
      "Average loss at step 31900: 1.780697 learning rate: 2.500000\n",
      "Minibatch perplexity: 5.89\n",
      "Validation set perplexity: 5.98\n",
      "Average loss at step 32000: 1.789649 learning rate: 2.500000\n",
      "Minibatch perplexity: 6.02\n",
      "================================================================================\n",
      "moving one nine four years in one eight who that seven three nine one univer he l\n",
      "wxms three zero seven in a vedied hossely led mebh into which the mirbilitional c\n",
      "two nine eight when the dock may also his have high armation in the power with on\n",
      "kponsach all one one eight seven three as the rocks as it crefered by the valuse \n",
      "ppers of malled from gosts in the becan conforman during trad charced by the wher\n",
      "================================================================================\n",
      "Validation set perplexity: 6.03\n",
      "Average loss at step 32100: 1.779904 learning rate: 2.500000\n",
      "Minibatch perplexity: 5.15\n",
      "Validation set perplexity: 6.09\n",
      "Average loss at step 32200: 1.811230 learning rate: 2.500000\n",
      "Minibatch perplexity: 5.43\n",
      "Validation set perplexity: 6.10\n",
      "Average loss at step 32300: 1.787824 learning rate: 2.500000\n",
      "Minibatch perplexity: 6.17\n",
      "Validation set perplexity: 6.09\n",
      "Average loss at step 32400: 1.799763 learning rate: 2.500000\n",
      "Minibatch perplexity: 6.14\n",
      "Validation set perplexity: 6.03\n",
      "Average loss at step 32500: 1.807366 learning rate: 2.500000\n",
      "Minibatch perplexity: 6.14\n",
      "Validation set perplexity: 6.03\n",
      "Average loss at step 32600: 1.771748 learning rate: 2.500000\n",
      "Minibatch perplexity: 6.01\n",
      "Validation set perplexity: 6.02\n",
      "Average loss at step 32700: 1.790371 learning rate: 2.500000\n",
      "Minibatch perplexity: 6.14\n",
      "Validation set perplexity: 5.99\n",
      "Average loss at step 32800: 1.780119 learning rate: 2.500000\n",
      "Minibatch perplexity: 6.45\n",
      "Validation set perplexity: 6.01\n",
      "Average loss at step 32900: 1.800795 learning rate: 2.500000\n",
      "Minibatch perplexity: 6.24\n",
      "Validation set perplexity: 5.97\n",
      "Average loss at step 33000: 1.783012 learning rate: 2.500000\n",
      "Minibatch perplexity: 5.26\n",
      "================================================================================\n",
      "gates guither in persortal proceist interprocessed addition had he orgically ke p\n",
      "ews in novident nearions in stroters to positive between casion quanker large tre\n",
      "droe they least and leapts reviser a pain historication them on the group in mexl\n",
      "dfins one one nine nine nine eight zero zero is the city the for a uses a right v\n",
      "you by d in the a the not the the laving the respreferent the aire island brotdic\n",
      "================================================================================\n",
      "Validation set perplexity: 6.07\n",
      "Average loss at step 33100: 1.782042 learning rate: 2.500000\n",
      "Minibatch perplexity: 5.69\n",
      "Validation set perplexity: 6.05\n",
      "Average loss at step 33200: 1.784138 learning rate: 2.500000\n",
      "Minibatch perplexity: 5.66\n",
      "Validation set perplexity: 6.10\n",
      "Average loss at step 33300: 1.791824 learning rate: 2.500000\n",
      "Minibatch perplexity: 5.88\n",
      "Validation set perplexity: 6.02\n",
      "Average loss at step 33400: 1.808942 learning rate: 2.500000\n",
      "Minibatch perplexity: 6.37\n",
      "Validation set perplexity: 6.06\n",
      "Average loss at step 33500: 1.784921 learning rate: 2.500000\n",
      "Minibatch perplexity: 6.09\n",
      "Validation set perplexity: 6.07\n",
      "Average loss at step 33600: 1.771761 learning rate: 2.500000\n",
      "Minibatch perplexity: 5.69\n",
      "Validation set perplexity: 5.95\n",
      "Average loss at step 33700: 1.765040 learning rate: 2.500000\n",
      "Minibatch perplexity: 6.51\n",
      "Validation set perplexity: 5.92\n",
      "Average loss at step 33800: 1.784701 learning rate: 2.500000\n",
      "Minibatch perplexity: 5.62\n",
      "Validation set perplexity: 5.97\n",
      "Average loss at step 33900: 1.768762 learning rate: 2.500000\n",
      "Minibatch perplexity: 5.65\n",
      "Validation set perplexity: 6.00\n",
      "Average loss at step 34000: 1.763420 learning rate: 2.500000\n",
      "Minibatch perplexity: 5.87\n",
      "================================================================================\n",
      " uneops lock ko the include striction to impresiticains of the histifying laright\n",
      "sbn three zero zero zero zero about marniam wall though the jame in one five dish\n",
      "wv sear collowed struction of the fewing may des dolowever beforence and example \n",
      "bir sativing have seven elect of jaise dulture lead of the pose disting as aughte\n",
      "fjefor actly will western for be hand quations may hook is bey hycleries reasing \n",
      "================================================================================\n",
      "Validation set perplexity: 5.94\n",
      "Average loss at step 34100: 1.769746 learning rate: 2.500000\n",
      "Minibatch perplexity: 6.02\n",
      "Validation set perplexity: 6.02\n",
      "Average loss at step 34200: 1.738514 learning rate: 2.500000\n",
      "Minibatch perplexity: 6.05\n",
      "Validation set perplexity: 5.96\n",
      "Average loss at step 34300: 1.718621 learning rate: 2.500000\n",
      "Minibatch perplexity: 6.38\n",
      "Validation set perplexity: 6.00\n",
      "Average loss at step 34400: 1.748413 learning rate: 2.500000\n",
      "Minibatch perplexity: 5.63\n",
      "Validation set perplexity: 5.97\n",
      "Average loss at step 34500: 1.790290 learning rate: 2.500000\n",
      "Minibatch perplexity: 5.18\n",
      "Validation set perplexity: 6.00\n",
      "Average loss at step 34600: 1.777461 learning rate: 2.500000\n",
      "Minibatch perplexity: 5.83\n",
      "Validation set perplexity: 6.08\n",
      "Average loss at step 34700: 1.786280 learning rate: 2.500000\n",
      "Minibatch perplexity: 5.77\n",
      "Validation set perplexity: 6.03\n",
      "Average loss at step 34800: 1.780960 learning rate: 2.500000\n",
      "Minibatch perplexity: 5.63\n",
      "Validation set perplexity: 5.98\n",
      "Average loss at step 34900: 1.787440 learning rate: 2.500000\n",
      "Minibatch perplexity: 6.84\n",
      "Validation set perplexity: 5.96\n",
      "Average loss at step 35000: 1.751419 learning rate: 2.500000\n",
      "Minibatch perplexity: 5.59\n",
      "================================================================================\n",
      "vw callected in the ic somem the one one nine five and and he aking scanalong his\n",
      "hful two eight eight confining chost is mayed via many press of the resust in joh\n",
      "constructes the new on nine nine four one two a pise univers that inventian and h\n",
      "bwebered the calso was has in the force that the cloused in part spaner king crea\n",
      "ydramed mides defer the was john may offormational applement providence neight ex\n",
      "================================================================================\n",
      "Validation set perplexity: 6.06\n",
      "Average loss at step 35100: 1.782436 learning rate: 2.500000\n",
      "Minibatch perplexity: 6.08\n",
      "Validation set perplexity: 6.01\n",
      "Average loss at step 35200: 1.767127 learning rate: 2.500000\n",
      "Minibatch perplexity: 5.61\n",
      "Validation set perplexity: 6.01\n",
      "Average loss at step 35300: 1.780109 learning rate: 2.500000\n",
      "Minibatch perplexity: 5.94\n",
      "Validation set perplexity: 6.03\n",
      "Average loss at step 35400: 1.770187 learning rate: 2.500000\n",
      "Minibatch perplexity: 6.32\n",
      "Validation set perplexity: 6.01\n",
      "Average loss at step 35500: 1.761961 learning rate: 2.500000\n",
      "Minibatch perplexity: 5.75\n",
      "Validation set perplexity: 6.01\n",
      "Average loss at step 35600: 1.768381 learning rate: 2.500000\n",
      "Minibatch perplexity: 5.99\n",
      "Validation set perplexity: 6.11\n",
      "Average loss at step 35700: 1.781319 learning rate: 2.500000\n",
      "Minibatch perplexity: 5.79\n",
      "Validation set perplexity: 6.08\n",
      "Average loss at step 35800: 1.764622 learning rate: 2.500000\n",
      "Minibatch perplexity: 7.24\n",
      "Validation set perplexity: 6.01\n",
      "Average loss at step 35900: 1.762139 learning rate: 2.500000\n",
      "Minibatch perplexity: 6.00\n",
      "Validation set perplexity: 6.00\n",
      "Average loss at step 36000: 1.775901 learning rate: 2.500000\n",
      "Minibatch perplexity: 5.24\n",
      "================================================================================\n",
      "x in the yor the trand was nothor popular changroups and cumber and so leth thoug\n",
      "jg name the vidents and the n a for san symple major mesland in refermanian cambe\n",
      "rform one one air one undon papany resident at the rolong fe degered publicite ca\n",
      "yjke recondes of the created to and rovation making was vid of meanon infaminal a\n",
      "fc equaring howed a stead assotion internal parted and dooted methough any one ni\n",
      "================================================================================\n",
      "Validation set perplexity: 5.99\n",
      "Average loss at step 36100: 1.765355 learning rate: 2.500000\n",
      "Minibatch perplexity: 5.86\n",
      "Validation set perplexity: 5.91\n",
      "Average loss at step 36200: 1.805498 learning rate: 2.500000\n",
      "Minibatch perplexity: 6.66\n",
      "Validation set perplexity: 6.03\n",
      "Average loss at step 36300: 1.775506 learning rate: 2.500000\n",
      "Minibatch perplexity: 5.28\n",
      "Validation set perplexity: 5.98\n",
      "Average loss at step 36400: 1.778335 learning rate: 2.500000\n",
      "Minibatch perplexity: 5.80\n",
      "Validation set perplexity: 6.02\n",
      "Average loss at step 36500: 1.768299 learning rate: 2.500000\n",
      "Minibatch perplexity: 5.73\n",
      "Validation set perplexity: 5.99\n",
      "Average loss at step 36600: 1.762690 learning rate: 2.500000\n",
      "Minibatch perplexity: 5.21\n",
      "Validation set perplexity: 5.99\n",
      "Average loss at step 36700: 1.784533 learning rate: 2.500000\n",
      "Minibatch perplexity: 5.87\n",
      "Validation set perplexity: 6.12\n",
      "Average loss at step 36800: 1.788004 learning rate: 2.500000\n",
      "Minibatch perplexity: 6.49\n",
      "Validation set perplexity: 6.15\n",
      "Average loss at step 36900: 1.806991 learning rate: 2.500000\n",
      "Minibatch perplexity: 6.18\n",
      "Validation set perplexity: 6.09\n",
      "Average loss at step 37000: 1.806284 learning rate: 2.500000\n",
      "Minibatch perplexity: 6.23\n",
      "================================================================================\n",
      "ken see the coon sonroyed from mish example and suby methato language continue an\n",
      "cious of all xplayorishis pence was law the saurch s was schave of their satinies\n",
      "bon in prose that and belluch parter deference cangth support reby has holyample \n",
      "ljohe factional huml he dary life past paoist programments havis becaused as main\n",
      "qbast unies deconned aposition of germany governing by american was frammunity on\n",
      "================================================================================\n",
      "Validation set perplexity: 6.13\n",
      "Average loss at step 37100: 1.792753 learning rate: 2.500000\n",
      "Minibatch perplexity: 5.77\n",
      "Validation set perplexity: 6.12\n",
      "Average loss at step 37200: 1.797498 learning rate: 2.500000\n",
      "Minibatch perplexity: 5.27\n",
      "Validation set perplexity: 6.08\n",
      "Average loss at step 37300: 1.804340 learning rate: 2.500000\n",
      "Minibatch perplexity: 5.88\n",
      "Validation set perplexity: 6.08\n",
      "Average loss at step 37400: 1.810600 learning rate: 2.500000\n",
      "Minibatch perplexity: 6.94\n",
      "Validation set perplexity: 6.15\n",
      "Average loss at step 37500: 1.831920 learning rate: 2.500000\n",
      "Minibatch perplexity: 5.52\n",
      "Validation set perplexity: 6.13\n",
      "Average loss at step 37600: 1.802569 learning rate: 2.500000\n",
      "Minibatch perplexity: 5.39\n",
      "Validation set perplexity: 6.11\n",
      "Average loss at step 37700: 1.774861 learning rate: 2.500000\n",
      "Minibatch perplexity: 5.90\n",
      "Validation set perplexity: 6.09\n",
      "Average loss at step 37800: 1.793830 learning rate: 2.500000\n",
      "Minibatch perplexity: 5.92\n",
      "Validation set perplexity: 6.10\n",
      "Average loss at step 37900: 1.796788 learning rate: 2.500000\n",
      "Minibatch perplexity: 5.41\n",
      "Validation set perplexity: 6.08\n",
      "Average loss at step 38000: 1.792526 learning rate: 2.500000\n",
      "Minibatch perplexity: 5.95\n",
      "================================================================================\n",
      "ty yearlet as compent distudy realli rope godded in a unoped to enhiropment of op\n",
      "uphy was internated made for from book from phich has in the population that as a\n",
      "xual marnia subgive in the and nation crail common hone seven four elether from i\n",
      "ad the remeding reciders many four nine eight six or affected facted a stord bega\n",
      "ese dagually and natural brower such one and the their they jother cus constanded\n",
      "================================================================================\n",
      "Validation set perplexity: 6.07\n",
      "Average loss at step 38100: 1.800403 learning rate: 2.500000\n",
      "Minibatch perplexity: 5.94\n",
      "Validation set perplexity: 6.14\n",
      "Average loss at step 38200: 1.815312 learning rate: 2.500000\n",
      "Minibatch perplexity: 6.05\n",
      "Validation set perplexity: 6.04\n",
      "Average loss at step 38300: 1.803783 learning rate: 2.500000\n",
      "Minibatch perplexity: 6.31\n",
      "Validation set perplexity: 6.08\n",
      "Average loss at step 38400: 1.803252 learning rate: 2.500000\n",
      "Minibatch perplexity: 6.05\n",
      "Validation set perplexity: 6.10\n",
      "Average loss at step 38500: 1.775863 learning rate: 2.500000\n",
      "Minibatch perplexity: 6.52\n",
      "Validation set perplexity: 6.09\n",
      "Average loss at step 38600: 1.772669 learning rate: 2.500000\n",
      "Minibatch perplexity: 5.64\n",
      "Validation set perplexity: 6.15\n",
      "Average loss at step 38700: 1.796775 learning rate: 2.500000\n",
      "Minibatch perplexity: 5.61\n",
      "Validation set perplexity: 6.11\n",
      "Average loss at step 38800: 1.788820 learning rate: 2.500000\n",
      "Minibatch perplexity: 6.41\n",
      "Validation set perplexity: 6.08\n",
      "Average loss at step 38900: 1.784933 learning rate: 2.500000\n",
      "Minibatch perplexity: 5.48\n",
      "Validation set perplexity: 6.06\n",
      "Average loss at step 39000: 1.796200 learning rate: 2.500000\n",
      "Minibatch perplexity: 6.39\n",
      "================================================================================\n",
      "with in one more galined of an orgapis and the prusitions who puotar white his fo\n",
      "jsociation interportics on universed by more cable example and only and unown be \n",
      "quidder the gox in their e pernation of their the efficed of post which droup sig\n",
      "ix four eight nine zero century of not in new contrences in the a cultures in dis\n",
      "even leginal prol and in mil brized that one nine seven american germaned the pro\n",
      "================================================================================\n",
      "Validation set perplexity: 6.05\n",
      "Average loss at step 39100: 1.797676 learning rate: 2.500000\n",
      "Minibatch perplexity: 5.73\n",
      "Validation set perplexity: 6.00\n",
      "Average loss at step 39200: 1.779103 learning rate: 2.500000\n",
      "Minibatch perplexity: 5.79\n",
      "Validation set perplexity: 6.02\n",
      "Average loss at step 39300: 1.765819 learning rate: 2.500000\n",
      "Minibatch perplexity: 5.80\n",
      "Validation set perplexity: 6.00\n",
      "Average loss at step 39400: 1.790008 learning rate: 2.500000\n",
      "Minibatch perplexity: 5.68\n",
      "Validation set perplexity: 5.99\n",
      "Average loss at step 39500: 1.780563 learning rate: 2.500000\n",
      "Minibatch perplexity: 5.63\n",
      "Validation set perplexity: 6.00\n",
      "Average loss at step 39600: 1.770355 learning rate: 2.500000\n",
      "Minibatch perplexity: 6.23\n",
      "Validation set perplexity: 6.00\n",
      "Average loss at step 39700: 1.775923 learning rate: 2.500000\n",
      "Minibatch perplexity: 5.78\n",
      "Validation set perplexity: 5.93\n",
      "Average loss at step 39800: 1.791390 learning rate: 2.500000\n",
      "Minibatch perplexity: 5.75\n",
      "Validation set perplexity: 6.00\n",
      "Average loss at step 39900: 1.781557 learning rate: 2.500000\n",
      "Minibatch perplexity: 5.84\n",
      "Validation set perplexity: 6.03\n",
      "Average loss at step 40000: 1.793985 learning rate: 1.250000\n",
      "Minibatch perplexity: 6.44\n",
      "================================================================================\n",
      "rt that the the mistical system was play callading a coccirection shav mitoring j\n",
      "bpaning seven four infrom dotoses resigning the cribury his been one nine seven h\n",
      "p down mayor film befing corname that country as first a cultury of one nine nine\n",
      "pinigo varial bel is decis liter chare texual he of exple for consides in the ope\n",
      "volulate from known recial at art was his justing of the ban include was ressible\n",
      "================================================================================\n",
      "Validation set perplexity: 6.02\n"
     ]
    }
   ],
   "source": [
    "num_steps = 40001\n",
    "summary_frequency = 100\n",
    "    \n",
    "with tf.Session(graph=graph) as session:\n",
    "  tf.initialize_all_variables().run()\n",
    "  print('Initialized')\n",
    "  mean_loss = 0\n",
    "  for step in range(num_steps):\n",
    "    batches = train_batches.next()\n",
    "    feed_dict = dict()\n",
    "    for i in range(num_unrollings-1):\n",
    "      feed_dict[train_inputs[i]] = convertOneHot(batches[i])*vocabulary_size + convertOneHot(batches[i+1])\n",
    "    for i in range(num_unrollings -1):\n",
    "      feed_dict[train_labels[i]] = batches[i+2]\n",
    "    feed_dict[keep_prob] = kp\n",
    "    _, l, predictions, lr = session.run(\n",
    "      [optimizer, loss, train_prediction, learning_rate], feed_dict=feed_dict)\n",
    "    mean_loss += l\n",
    "    if step % summary_frequency == 0:\n",
    "      if step > 0:\n",
    "        mean_loss = mean_loss / summary_frequency\n",
    "      # The mean loss is an estimate of the loss over the last few batches.\n",
    "      print(\n",
    "        'Average loss at step %d: %f learning rate: %f' % (step, mean_loss, lr))\n",
    "      mean_loss = 0\n",
    "      labels = np.concatenate(list(batches)[2:])\n",
    "      print('Minibatch perplexity: %.2f' % float(\n",
    "        np.exp(logprob(predictions, labels))))\n",
    "      if step % (summary_frequency * 10) == 0:\n",
    "        # Generate some samples.\n",
    "        print('=' * 80)\n",
    "        for _ in range(5):\n",
    "          feed1 = sample(random_distribution())\n",
    "          sentence = characters(feed1)[0]\n",
    "          feed2 = sample(random_distribution())\n",
    "          sentence += characters(feed2)[0]\n",
    "          reset_sample_state.run()\n",
    "          for _ in range(79):\n",
    "            prediction = sample_prediction.eval({sample_input_1: convertOneHot(feed1)*vocabulary_size + \n",
    "                                                convertOneHot(feed2), keep_prob: 1.0})\n",
    "            feed1 = feed2\n",
    "            feed2 = sample(prediction)\n",
    "            sentence += characters(feed2)[0]\n",
    "          print(sentence)\n",
    "        print('=' * 80)\n",
    "      # Measure validation set perplexity.\n",
    "      reset_sample_state.run()\n",
    "      valid_logprob = 0\n",
    "      for _ in range(valid_size):\n",
    "        b = valid_batches.next()\n",
    "        predictions = sample_prediction.eval({sample_input_1: convertOneHot(b[0])*vocabulary_size\n",
    "                                              + convertOneHot(b[1]), keep_prob: 1.0})\n",
    "        valid_logprob = valid_logprob + logprob(predictions, b[2])\n",
    "      print('Validation set perplexity: %.2f' % float(np.exp(\n",
    "        valid_logprob / valid_size)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Alternative model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initialized\n",
      "Average loss at step 0: 3.326601 learning rate: 20.000000\n",
      "Minibatch perplexity: 27.84\n",
      "================================================================================\n",
      "ks i pfn acp e      g meir w a  to ef    ma oen r s  ln gleke  ti s d  e it g  l \n",
      "kbeo  atpn  t b  e   i   l i   ar e   bnt  ria niek l a    vgy ie   rt ir sm rpi \n",
      "lp jtn  ejn m ek en i  h yg  i s   o t oe   iuli t  aao        mny rwuz ii   eh  \n",
      "ta iuk  dmhfd   to d   an  y   atc n oim i  e  id   ni p taan njt  d  draz est i \n",
      "gf  ti    naa   caneee e  a ee nqm e     oe  e  wn nir  a wmhe an t ilsio     e  \n",
      "================================================================================\n",
      "Validation set perplexity: 22.08\n",
      "Average loss at step 100: 2.878362 learning rate: 20.000000\n",
      "Minibatch perplexity: 14.04\n",
      "Validation set perplexity: 13.46\n",
      "Average loss at step 200: 2.689616 learning rate: 20.000000\n",
      "Minibatch perplexity: 12.13\n",
      "Validation set perplexity: 13.38\n",
      "Average loss at step 300: 2.630080 learning rate: 20.000000\n",
      "Minibatch perplexity: 12.78\n",
      "Validation set perplexity: 13.92\n",
      "Average loss at step 400: 2.571905 learning rate: 20.000000\n",
      "Minibatch perplexity: 17.16\n",
      "Validation set perplexity: 12.89\n",
      "Average loss at step 500: 2.548166 learning rate: 20.000000\n",
      "Minibatch perplexity: 11.49\n",
      "Validation set perplexity: 13.13\n",
      "Average loss at step 600: 2.549876 learning rate: 20.000000\n",
      "Minibatch perplexity: 11.12\n",
      "Validation set perplexity: 14.41\n",
      "Average loss at step 700: 2.511912 learning rate: 20.000000\n",
      "Minibatch perplexity: 11.93\n",
      "Validation set perplexity: 11.95\n",
      "Average loss at step 800: 2.518340 learning rate: 20.000000\n",
      "Minibatch perplexity: 11.07\n",
      "Validation set perplexity: 12.37\n",
      "Average loss at step 900: 2.503939 learning rate: 20.000000\n",
      "Minibatch perplexity: 14.81\n",
      "Validation set perplexity: 12.98\n",
      "Average loss at step 1000: 2.515960 learning rate: 20.000000\n",
      "Minibatch perplexity: 12.00\n",
      "================================================================================\n",
      "prom one diant the eqlitten an the birl sor dael acu cpron ho fitne the uldais ri\n",
      "cy thait of cops daat diter of the wis in wero nive for h in cow as wips rit ha n\n",
      "uunt of wor our to mocmalt cove nind bomaaght jon achas pris cwen the por achitwo\n",
      "ns the favivne a pa  fe fho fo fencroaro nat hove let hit hit of of of dkon busga\n",
      "ci the zero for a rruzera the of he spas jumbut ninn siroy c haly the a  the yor \n",
      "================================================================================\n",
      "Validation set perplexity: 11.92\n",
      "Average loss at step 1100: 2.495151 learning rate: 20.000000\n",
      "Minibatch perplexity: 11.17\n",
      "Validation set perplexity: 12.32\n",
      "Average loss at step 1200: 2.467510 learning rate: 20.000000\n",
      "Minibatch perplexity: 12.57\n",
      "Validation set perplexity: 14.29\n",
      "Average loss at step 1300: 2.487284 learning rate: 20.000000\n",
      "Minibatch perplexity: 10.84\n",
      "Validation set perplexity: 12.49\n",
      "Average loss at step 1400: 2.483707 learning rate: 20.000000\n",
      "Minibatch perplexity: 10.58\n",
      "Validation set perplexity: 11.54\n",
      "Average loss at step 1500: 2.477071 learning rate: 20.000000\n",
      "Minibatch perplexity: 12.84\n",
      "Validation set perplexity: 11.24\n",
      "Average loss at step 1600: 2.496977 learning rate: 20.000000\n",
      "Minibatch perplexity: 15.52\n",
      "Validation set perplexity: 12.05\n",
      "Average loss at step 1700: 2.493597 learning rate: 20.000000\n",
      "Minibatch perplexity: 11.83\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-39-851aa817dd08>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    152\u001b[0m         \u001b[0mb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mvalid_batches\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    153\u001b[0m         predictions = sample_prediction.eval({sample_input_1: b[0],\n\u001b[0;32m--> 154\u001b[0;31m                                                sample_input_2: b[1], keep_prob: 1.0})\n\u001b[0m\u001b[1;32m    155\u001b[0m         \u001b[0mvalid_logprob\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mvalid_logprob\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mlogprob\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpredictions\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mb\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    156\u001b[0m       print('Validation set perplexity: %.2f' % float(np.exp(\n",
      "\u001b[0;32m/Users/ykchia/anaconda/lib/python2.7/site-packages/tensorflow/python/framework/ops.pyc\u001b[0m in \u001b[0;36meval\u001b[0;34m(self, feed_dict, session)\u001b[0m\n\u001b[1;32m    553\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    554\u001b[0m     \"\"\"\n\u001b[0;32m--> 555\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0m_eval_using_default_session\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgraph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msession\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    556\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    557\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/ykchia/anaconda/lib/python2.7/site-packages/tensorflow/python/framework/ops.pyc\u001b[0m in \u001b[0;36m_eval_using_default_session\u001b[0;34m(tensors, feed_dict, graph, session)\u001b[0m\n\u001b[1;32m   3496\u001b[0m                        \u001b[0;34m\"the tensor's graph is different from the session's \"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3497\u001b[0m                        \"graph.\")\n\u001b[0;32m-> 3498\u001b[0;31m   \u001b[0;32mreturn\u001b[0m \u001b[0msession\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3499\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3500\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/ykchia/anaconda/lib/python2.7/site-packages/tensorflow/python/client/session.pyc\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    370\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    371\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[0;32m--> 372\u001b[0;31m                          run_metadata_ptr)\n\u001b[0m\u001b[1;32m    373\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    374\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/ykchia/anaconda/lib/python2.7/site-packages/tensorflow/python/client/session.pyc\u001b[0m in \u001b[0;36m_run\u001b[0;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    634\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    635\u001b[0m       results = self._do_run(handle, target_list, unique_fetches,\n\u001b[0;32m--> 636\u001b[0;31m                              feed_dict_string, options, run_metadata)\n\u001b[0m\u001b[1;32m    637\u001b[0m     \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    638\u001b[0m       \u001b[0;31m# The movers are no longer used. Delete them.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/ykchia/anaconda/lib/python2.7/site-packages/tensorflow/python/client/session.pyc\u001b[0m in \u001b[0;36m_do_run\u001b[0;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    706\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    707\u001b[0m       return self._do_call(_run_fn, self._session, feed_dict, fetch_list,\n\u001b[0;32m--> 708\u001b[0;31m                            target_list, options, run_metadata)\n\u001b[0m\u001b[1;32m    709\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    710\u001b[0m       return self._do_call(_prun_fn, self._session, handle, feed_dict,\n",
      "\u001b[0;32m/Users/ykchia/anaconda/lib/python2.7/site-packages/tensorflow/python/client/session.pyc\u001b[0m in \u001b[0;36m_do_call\u001b[0;34m(self, fn, *args)\u001b[0m\n\u001b[1;32m    713\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    714\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 715\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    716\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mOpError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    717\u001b[0m       \u001b[0mmessage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcompat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmessage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/ykchia/anaconda/lib/python2.7/site-packages/tensorflow/python/client/session.pyc\u001b[0m in \u001b[0;36m_run_fn\u001b[0;34m(session, feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[1;32m    695\u001b[0m         return tf_session.TF_Run(session, options,\n\u001b[1;32m    696\u001b[0m                                  \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_list\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 697\u001b[0;31m                                  status, run_metadata)\n\u001b[0m\u001b[1;32m    698\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    699\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_prun_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msession\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "num_nodes = 128\n",
    "# construct bigram by using embedding for each character.\n",
    "embed_dim = 32\n",
    "\n",
    "batch_size=64\n",
    "num_unrollings=10\n",
    "kp = 0.5\n",
    "train_batches = BatchGenerator(train_text, batch_size, num_unrollings)\n",
    "valid_batches = BatchGenerator(valid_text, 1, 2)\n",
    "\n",
    "graph = tf.Graph()\n",
    "with graph.as_default():\n",
    "  embedMat = tf.Variable(\n",
    "    tf.random_uniform([vocabulary_size, embed_dim], -1.0, 1.0))\n",
    "  inMat = tf.Variable(tf.truncated_normal([embed_dim, 4*num_nodes], -0.1, 0.1))\n",
    "  outMat = tf.Variable(tf.truncated_normal([num_nodes, 4*num_nodes], -0.1, 0.1))\n",
    "  bias =  tf.Variable(tf.zeros([1,4*num_nodes])) \n",
    "  keep_prob = tf.placeholder(\"float\")\n",
    "  # Variables saving state across unrollings.\n",
    "  saved_output = tf.Variable(tf.zeros([batch_size, num_nodes]), trainable=False)\n",
    "  saved_state = tf.Variable(tf.zeros([batch_size, num_nodes]), trainable=False)\n",
    "  # Classifier weights and biases.\n",
    "  w = tf.Variable(tf.truncated_normal([num_nodes, vocabulary_size], -0.1, 0.1))\n",
    "  b = tf.Variable(tf.zeros([vocabulary_size]))\n",
    "  # Definition of the cell computation.\n",
    "  def lstm_cell(i, o, state):\n",
    "    \"\"\"Create a LSTM cell. See e.g.: http://arxiv.org/pdf/1402.1128v1.pdf\n",
    "    Note that in this formulation, we omit the various connections between the\n",
    "    previous state and the gates.\"\"\"\n",
    "    overallIn = tf.nn.dropout(tf.matmul(i, inMat),keep_prob)\n",
    "    #print(overallIn[0,0:10])\n",
    "    #print(overallIn[:,0:10])  \n",
    "    overallOut = tf.matmul(o, outMat)\n",
    "    input_gate = tf.sigmoid(overallIn[:,0:num_nodes] + \n",
    "                             overallOut[:,0:num_nodes] + bias[:,0:num_nodes])\n",
    "    forget_gate = tf.sigmoid(overallIn[:,num_nodes:2*num_nodes] + \n",
    "                             overallOut[:,num_nodes:2*num_nodes] + bias[:,num_nodes:2*num_nodes])\n",
    "    update = (overallIn[:,2*num_nodes:3*num_nodes] + \n",
    "                             overallOut[:,2*num_nodes:3*num_nodes] + bias[:,2*num_nodes:3*num_nodes])\n",
    "    state = forget_gate * state + input_gate * tf.tanh(update)\n",
    "    output_gate = tf.sigmoid(overallIn[:,3*num_nodes:4*num_nodes] + \n",
    "                             overallOut[:,3*num_nodes:4*num_nodes] + bias[:,3*num_nodes:4*num_nodes])\n",
    "    return tf.nn.dropout(output_gate * tf.tanh(state),keep_prob), state\n",
    "\n",
    "  # Input data.\n",
    "  train_inputs = list()\n",
    "  train_labels = list()\n",
    "  for _ in range(num_unrollings):\n",
    "    train_inputs.append(\n",
    "      tf.placeholder(tf.float32, shape=[batch_size, vocabulary_size]))\n",
    "  for _ in range(num_unrollings-1):\n",
    "    train_labels.append(\n",
    "      tf.placeholder(tf.float32, shape=[batch_size,vocabulary_size]))  \n",
    "    \n",
    "  # Unrolled LSTM loop.\n",
    "  outputs = list()\n",
    "  output = saved_output\n",
    "  state = saved_state\n",
    "  for i in range(num_unrollings-1):\n",
    "    #in1 = tf.nn.embedding_lookup(embedMat, train_inputs[i])\n",
    "    #print(tf.shape(in1))\n",
    "    in1 = tf.matmul(train_inputs[i], embedMat) + tf.matmul(train_inputs[i+1], embedMat)\n",
    "    output, state = lstm_cell(in1, output, state)\n",
    "    outputs.append(output)\n",
    "\n",
    "  # State saving across unrollings.\n",
    "  with tf.control_dependencies([saved_output.assign(output),\n",
    "                                saved_state.assign(state)]):\n",
    "    # Classifier.\n",
    "    logits = tf.nn.xw_plus_b(tf.concat(0, outputs), w, b)\n",
    "    loss = tf.reduce_mean(\n",
    "      tf.nn.softmax_cross_entropy_with_logits(\n",
    "        logits, tf.concat(0, train_labels)))\n",
    "\n",
    "  # Optimizer.\n",
    "  global_step = tf.Variable(0)\n",
    "  learning_rate = tf.train.exponential_decay(\n",
    "    20.0, global_step, 10000, 0.5, staircase=True)\n",
    "  optimizer = tf.train.GradientDescentOptimizer(learning_rate)\n",
    "  gradients, v = zip(*optimizer.compute_gradients(loss))\n",
    "  gradients, _ = tf.clip_by_global_norm(gradients, 1.25)\n",
    "  optimizer = optimizer.apply_gradients(\n",
    "    zip(gradients, v), global_step=global_step)\n",
    "\n",
    "  # Predictions.\n",
    "  train_prediction = tf.nn.softmax(logits)\n",
    "  \n",
    "  # Sampling and validation eval: batch 1, no unrolling.\n",
    "  sample_input_1 = tf.placeholder(tf.float32, shape=[1, vocabulary_size])\n",
    "  sample_input_2 = tf.placeholder(tf.float32, shape=[1,vocabulary_size])\n",
    "  #sample_in = tf.nn.embedding_lookup(embedMat, sample_input_1)\n",
    "  sample_in = tf.matmul(sample_input_1, embedMat)+tf.matmul(sample_input_2, embedMat)  \n",
    "  saved_sample_output = tf.Variable(tf.zeros([1, num_nodes]))\n",
    "  saved_sample_state = tf.Variable(tf.zeros([1, num_nodes]))\n",
    "  reset_sample_state = tf.group(\n",
    "    saved_sample_output.assign(tf.zeros([1, num_nodes])),\n",
    "    saved_sample_state.assign(tf.zeros([1, num_nodes])))\n",
    "  sample_output, sample_state = lstm_cell(\n",
    "    sample_in, saved_sample_output, saved_sample_state)\n",
    "  with tf.control_dependencies([saved_sample_output.assign(sample_output),\n",
    "                                saved_sample_state.assign(sample_state)]):\n",
    "    sample_prediction = tf.nn.softmax(tf.nn.xw_plus_b(sample_output, w, b))\n",
    "\n",
    "num_steps = 30001\n",
    "summary_frequency = 100\n",
    "    \n",
    "with tf.Session(graph=graph) as session:\n",
    "  tf.initialize_all_variables().run()\n",
    "  print('Initialized')\n",
    "  mean_loss = 0\n",
    "  for step in range(num_steps):\n",
    "    batches = train_batches.next()\n",
    "    feed_dict = dict()\n",
    "    for i in range(num_unrollings):\n",
    "      feed_dict[train_inputs[i]] = batches[i]\n",
    "    for i in range(num_unrollings -1):\n",
    "      feed_dict[train_labels[i]] = batches[i+2]\n",
    "    feed_dict[keep_prob] = kp\n",
    "    _, l, predictions, lr = session.run(\n",
    "      [optimizer, loss, train_prediction, learning_rate], feed_dict=feed_dict)\n",
    "    mean_loss += l\n",
    "    if step % summary_frequency == 0:\n",
    "      if step > 0:\n",
    "        mean_loss = mean_loss / summary_frequency\n",
    "      # The mean loss is an estimate of the loss over the last few batches.\n",
    "      print(\n",
    "        'Average loss at step %d: %f learning rate: %f' % (step, mean_loss, lr))\n",
    "      mean_loss = 0\n",
    "      labels = np.concatenate(list(batches)[2:])\n",
    "      print('Minibatch perplexity: %.2f' % float(\n",
    "        np.exp(logprob(predictions, labels))))\n",
    "      if step % (summary_frequency * 10) == 0:\n",
    "        # Generate some samples.\n",
    "        print('=' * 80)\n",
    "        for _ in range(5):\n",
    "          feed1 = sample(random_distribution())\n",
    "          sentence = characters(feed1)[0]\n",
    "          feed2 = sample(random_distribution())\n",
    "          sentence += characters(feed2)[0]\n",
    "          reset_sample_state.run()\n",
    "          for _ in range(79):\n",
    "            prediction = sample_prediction.eval({sample_input_1: feed1, sample_input_2: feed2, keep_prob: 1.0})\n",
    "            feed1 = feed2\n",
    "            feed2 = sample(prediction)\n",
    "            sentence += characters(feed2)[0]\n",
    "          print(sentence)\n",
    "        print('=' * 80)\n",
    "      # Measure validation set perplexity.\n",
    "      reset_sample_state.run()\n",
    "      valid_logprob = 0\n",
    "      for _ in range(valid_size):\n",
    "        b = valid_batches.next()\n",
    "        predictions = sample_prediction.eval({sample_input_1: b[0],\n",
    "                                               sample_input_2: b[1], keep_prob: 1.0})\n",
    "        valid_logprob = valid_logprob + logprob(predictions, b[2])\n",
    "      print('Validation set perplexity: %.2f' % float(np.exp(\n",
    "        valid_logprob / valid_size)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Y5tapX3kpcqZ"
   },
   "source": [
    "---\n",
    "Problem 3\n",
    "---------\n",
    "\n",
    "(difficult!)\n",
    "\n",
    "Write a sequence-to-sequence LSTM which mirrors all the words in a sentence. For example, if your input is:\n",
    "\n",
    "    the quick brown fox\n",
    "    \n",
    "the model should attempt to output:\n",
    "\n",
    "    eht kciuq nworb xof\n",
    "    \n",
    "Refer to the lecture on how to put together a sequence-to-sequence model, as well as [this article](http://arxiv.org/abs/1409.3215) for best practices.\n",
    "\n",
    "---"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "default_view": {},
   "name": "6_lstm.ipynb",
   "provenance": [],
   "version": "0.3.2",
   "views": {}
  },
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
